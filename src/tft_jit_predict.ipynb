{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c383b17d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.7310, -0.7871, -0.7402, -0.6085, -0.7669, -0.8404, -0.6197, -0.8075,\n",
       "         -0.7866, -0.8444]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import pickle\n",
    "import torch\n",
    "\n",
    "\"\"\"\n",
    "feature map is like this:\n",
    "{'static_feats_numeric': ['latitude', 'longitude'],\n",
    " 'static_feats_categorical': [],\n",
    " 'historical_ts_numeric': ['tmin',\n",
    "  'tmax',\n",
    "  'precipitation',\n",
    "  'radiation',\n",
    "  'photoperiod',\n",
    "  'swvl1',\n",
    "  'doy',\n",
    "  'sif_clear_inst'],\n",
    " 'historical_ts_categorical': [],\n",
    " 'future_ts_numeric': ['doy'],\n",
    " 'future_ts_categorical': []}\n",
    "\"\"\"\n",
    "#### DATA \n",
    "B, T_hist, T_fut = 1, 60, 10\n",
    "# static\n",
    "static_num = torch.randn(B, 2)        #lat,long   \n",
    "static_cat = torch.empty(B, 0, dtype=torch.long)\n",
    "# historical\n",
    "hist_num   = torch.randn(B, T_hist, 8) # tmin, tmax, prcp, srad, swc, photoperiod, doy, lai\n",
    "hist_cat   = torch.empty(B, 0, dtype=torch.long)\n",
    "# future\n",
    "fut_num    = torch.randn(B, T_fut, 1)  \n",
    "fut_cat    = torch.empty(B, 0, dtype=torch.long)\n",
    "\n",
    "#### Model Path\n",
    "checkpoint_path = \"/glade/u/home/ayal/phenology-ml-clm/models/tft_scripted.pt\"\n",
    "#### Model Configuration\n",
    "configuration_test = {\n",
    "    \"task_type\": \"regression\",\n",
    "    \"target_window_start\": None,\n",
    "    \"data_props\": {'num_historical_numeric': 8, #tmin, tmax, prcp, srad, swc, photoperiod, doy, lai,\n",
    "  'num_historical_categorical': 0,\n",
    "  'num_static_numeric': 2, # lat, lon\n",
    "  'num_static_categorical': 0,\n",
    "  'num_future_numeric': 1, # doy\n",
    "  'num_future_categorical': 0,\n",
    "  'historical_categorical_cardinalities': [],\n",
    "  'static_categorical_cardinalities': [],\n",
    "  'future_categorical_cardinalities': []},\n",
    "    \"model\": {\n",
    "        \"attention_heads\": 4,\n",
    "        \"dropout\": 0.2,\n",
    "        \"lstm_layers\": 4,\n",
    "        \"state_size\": 160,\n",
    "        \"output_quantiles\": [0.1, 0.5, 0.9],\n",
    "    },\n",
    "}\n",
    "\n",
    "### Load the model\n",
    "model = torch.jit.load(checkpoint_path, map_location=\"cpu\")\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    # returns (B, T_fut, 3)\n",
    "    out = model(static_num,\n",
    "                   static_cat,\n",
    "                   hist_num,\n",
    "                   hist_cat,\n",
    "                   fut_num,\n",
    "                   fut_cat)\n",
    "    \n",
    "out[:, :, 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3fba4be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RecursiveScriptModule(original_name=TemporalFusionTransformer)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import pickle\n",
    "import torch\n",
    "\n",
    "\"\"\"\n",
    "feature map is like this:\n",
    "{'static_feats_numeric': ['latitude', 'longitude'],\n",
    " 'static_feats_categorical': [],\n",
    " 'historical_ts_numeric': ['tmin',\n",
    "  'tmax',\n",
    "  'precipitation',\n",
    "  'radiation',\n",
    "  'photoperiod',\n",
    "  'swvl1',\n",
    "  'doy',\n",
    "  'sif_clear_inst'],\n",
    " 'historical_ts_categorical': [],\n",
    " 'future_ts_numeric': ['doy'],\n",
    " 'future_ts_categorical': []}\n",
    "\"\"\"\n",
    "#### DATA \n",
    "B, T_hist, T_fut = 1, 60, 10\n",
    "# static\n",
    "static_num = torch.randn(B, 2)        #lat,long   \n",
    "static_cat = torch.empty(B, 0, dtype=torch.long)\n",
    "# historical\n",
    "hist_num   = torch.randn(B, T_hist, 8) # tmin, tmax, prcp, srad, swc, photoperiod, doy, lai\n",
    "hist_cat   = torch.empty(B, 0, dtype=torch.long)\n",
    "# future\n",
    "fut_num    = torch.randn(B, T_fut, 1)  \n",
    "fut_cat    = torch.empty(B, 0, dtype=torch.long)\n",
    "\n",
    "#### Model Path\n",
    "checkpoint_path = \"/glade/u/home/ayal/phenology-ml-clm/models/tft_scripted.pt\"\n",
    "#### Model Configuration\n",
    "configuration_test = {\n",
    "    \"task_type\": \"regression\",\n",
    "    \"target_window_start\": None,\n",
    "    \"data_props\": {'num_historical_numeric': 8, #tmin, tmax, prcp, srad, swc, photoperiod, doy, lai,\n",
    "  'num_historical_categorical': 0,\n",
    "  'num_static_numeric': 2, # lat, lon\n",
    "  'num_static_categorical': 0,\n",
    "  'num_future_numeric': 1, # doy\n",
    "  'num_future_categorical': 0,\n",
    "  'historical_categorical_cardinalities': [],\n",
    "  'static_categorical_cardinalities': [],\n",
    "  'future_categorical_cardinalities': []},\n",
    "    \"model\": {\n",
    "        \"attention_heads\": 4,\n",
    "        \"dropout\": 0.2,\n",
    "        \"lstm_layers\": 4,\n",
    "        \"state_size\": 160,\n",
    "        \"output_quantiles\": [0.1, 0.5, 0.9],\n",
    "    },\n",
    "}\n",
    "\n",
    "### Load the model\n",
    "model = torch.jit.load(checkpoint_path, map_location=\"cpu\")\n",
    "#model.eval()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54cd041e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "class OneArgWrapper(torch.nn.Module):\n",
    "    def __init__(self, model: torch.jit.ScriptModule):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        all_inputs: Tuple[\n",
    "            torch.Tensor, torch.Tensor,\n",
    "            torch.Tensor, torch.Tensor,\n",
    "            torch.Tensor, torch.Tensor\n",
    "        ]\n",
    "    ) -> torch.Tensor:\n",
    "        s_num, s_cat, h_num, h_cat, f_num, f_cat = all_inputs\n",
    "        return self.model(s_num, s_cat, h_num, h_cat, f_num, f_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69ea53af",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapper = OneArgWrapper(torch.jit.load(checkpoint_path, map_location=\"cpu\"))\n",
    "wrapper.eval()\n",
    "scripted = torch.jit.script(wrapper)\n",
    "model_frozen = torch.jit.freeze(scripted)\n",
    "out_file= \"/glade/u/home/ayal/phenology-ml-clm/models/tft_onearg.pt\"\n",
    "torch.jit.save(model_frozen, out_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb19a469",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RecursiveScriptModule(original_name=OneArgWrapper)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. Later, load it back exactly as a ScriptModule\n",
    "loaded = torch.jit.load(out_file, map_location=\"cpu\")\n",
    "loaded.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    # returns (B, T_fut, 3)\n",
    "    out = loaded(static_num,\n",
    "                   static_cat,\n",
    "                   hist_num,\n",
    "                   hist_cat,\n",
    "                   fut_num,\n",
    "                   fut_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6dbc1add",
   "metadata": {},
   "outputs": [],
   "source": [
    "tup = (static_num, static_cat, hist_num, hist_cat, fut_num, fut_cat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "069a68c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    # returns (B, T_fut, 3)\n",
    "    out = loaded(tup)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bea30b95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.7272, -0.7002,  0.1767],\n",
       "         [-0.7929, -0.7575,  0.0170],\n",
       "         [-0.8474, -0.7602, -0.0192],\n",
       "         [-0.7739, -0.7363, -0.1360],\n",
       "         [-0.8664, -0.7430, -0.0207],\n",
       "         [-0.5299, -0.5162, -0.1651],\n",
       "         [-0.4806, -0.3817,  0.0702],\n",
       "         [-0.5552, -0.4563,  0.0342],\n",
       "         [-0.5705, -0.4498,  0.0450],\n",
       "         [-0.8032, -0.7379,  0.0134]]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f412f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
