{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0597206-0df4-4ab8-a595-b74fd164a91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pickle\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas.api.types as ptypes\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import QuantileTransformer, LabelEncoder, StandardScaler, MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84596c34-8fd6-4e8f-b865-4fe8216d4bb6",
   "metadata": {},
   "source": [
    "# helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0cde000-430b-4996-a803-7b691706eaad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_loc_time(data_path: str, output_path: str):\n",
    "    \"\"\"\n",
    "    Sort the data by location and time and save to new file.    \n",
    "    Args: data_path: str, the path to the data file \n",
    "          output_path: str, path where to save the sorted file \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    file_name=data_path.split('/')[-1]\n",
    "    data_df= pd.read_parquet(data_path)\n",
    "    sorting_order = {\n",
    "        'location': 'asc',  # descending order\n",
    "        'time': 'asc',   # ascending order\n",
    "    }\n",
    "    # Convert the sorting order to ascending/descending values\n",
    "    sorting_values = {col: (True if order == 'asc' else False) for col, order in sorting_order.items()}\n",
    "    # Sort the DataFrame based on the specified columns and order\n",
    "    data_df = data_df.sort_values(by=list(sorting_order.keys()), ascending=list(sorting_values.values()))\n",
    "    data_df.to_parquet(output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3852c4ce-71e6-4a81-a872-e3cfd40b4816",
   "metadata": {},
   "source": [
    "# other "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c26165-7885-4390-8b0b-77f781f3c4a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0250c6-523f-4f4d-9036-6524e3f5fb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def tft_process(path, hist_len, fut_len, output_path):\n",
    "    \"\"\"\n",
    "    Process the data for TFT model training and save to new file.\n",
    "    \n",
    "    Args:\n",
    "    path: str, the path to the data file\n",
    "    hist_len: int, the length of the historical time series\n",
    "    fut_len: int, the length of the future time series\n",
    "    output_path: str, the path to the output file\n",
    "    output_filename: str, the name of the output file\n",
    "    \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    \n",
    "    output_filename = path.split('/')[-1].split('.')[0] + '.pkl'\n",
    "    data_df= pd.read_parquet(path)\n",
    "    print(data_df.columns)\n",
    "    #validation columns before dropping na \n",
    "    data_df = data_df[['time', 'location', 'latitude', 'longitude', 'tmin', 'tmax', 'precipitation', 'radiation', 'photoperiod', 'swvl1', 'sif_clear_inst', 'soil']]\n",
    "    #print size of\n",
    "    print(\"length of sorted parquet file\",len(data_df))\n",
    "    data_df = data_df.dropna()\n",
    "    \n",
    "    #data_df=data_df[data_df['latitude']>0]# northern hemisphere only \n",
    "    data_df['time'] = pd.to_datetime(data_df['time'])\n",
    "    # No records will be considered outside these bounds\n",
    "    start_date = datetime(1982, 1, 14)\n",
    "    end_date = datetime(2022, 1, 1)\n",
    "    \n",
    "    print(data_df['time'].min())\n",
    "    print(data_df['time'].max()) \n",
    "    \n",
    "    # these will not be included as part of the input data which will end up feeding the model\n",
    "    meta_attrs = ['time', 'location', \"soil_x\", \"soil_y\", \"id\"]\n",
    "    \n",
    "    # These are the variables that are known in advance, and will compose the futuristic time-series\n",
    "    known_attrs = ['tmin', 'tmax',  'radiation','precipitation', 'swvl1','photoperiod']\n",
    "    # The following set of variables will be considered as static, i.e. containing non-temporal information\n",
    "    # every attribute which is not listed here will be considered as temporal.\n",
    "    static_attrs = ['latitude', 'longitude', 'soil']\n",
    "    # The following set of variables will be considered as categorical.\n",
    "    # The rest of the variables (which are not listed below) will be considered as numeric.\n",
    "    categorical_attrs = [\"soil\"] #soil used to be here \n",
    "    target_signal = 'sif_clear_inst'\n",
    "\n",
    "    unique_locations = data_df['location'].unique()\n",
    "    print(str(unique_locations)+\" unique locations.\")\n",
    "    \n",
    "    \n",
    "    all_cols = list(data_df.columns)\n",
    "    feature_cols = [col for col in all_cols if col not in meta_attrs]\n",
    "    \n",
    "    feature_map = {\n",
    "        'static_feats_numeric': [col for col in feature_cols if col in static_attrs and col not in categorical_attrs],\n",
    "        'static_feats_categorical': [col for col in feature_cols if col in static_attrs and col in categorical_attrs],\n",
    "        'historical_ts_numeric': [col for col in feature_cols if col not in static_attrs and col not in categorical_attrs],\n",
    "        'historical_ts_categorical': [col for col in feature_cols if col not in static_attrs and col in categorical_attrs],\n",
    "        'future_ts_numeric': [col for col in feature_cols if col in known_attrs and col not in categorical_attrs],\n",
    "        'future_ts_categorical': [col for col in feature_cols if col in known_attrs and col in categorical_attrs]\n",
    "    }\n",
    "    # allocate a dictionary to contain the scaler and encoder objects after fitting them\n",
    "    scalers = {'numeric': dict(), 'categorical': dict()}\n",
    "    # for the categorical variables we would like to keep the cardinalities (how many categories for each variable)\n",
    "    categorical_cardinalities = dict()\n",
    "    \n",
    "    only_train= data_df\n",
    "    \n",
    "    for col in tqdm(feature_cols):\n",
    "        if col in categorical_attrs:\n",
    "            scalers['categorical'][col] = LabelEncoder().fit(only_train[col].values)\n",
    "            categorical_cardinalities[col] = only_train[col].nunique()\n",
    "        else:\n",
    "            if col in ['sif_clear_inst']:\n",
    "                scalers['numeric'][col] = StandardScaler().fit(only_train[col].values.astype(float).reshape(-1, 1))\n",
    "            elif col in ['day_of_year']:\n",
    "                scalers['numeric'][col] = MinMaxScaler().fit(only_train[col].values.astype(float).reshape(-1, 1))\n",
    "            else:\n",
    "                scalers['numeric'][col] = QuantileTransformer(n_quantiles=256).fit(\n",
    "                    only_train[col].values.astype(float).reshape(-1, 1))\n",
    "                \n",
    "    for col in tqdm(feature_cols):\n",
    "        if col in categorical_attrs:\n",
    "            #le = scalers['categorical'][col]\n",
    "            # handle cases with unseen keys\n",
    "            #le_dict = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "            #data_df[col] = data_df[col].apply(lambda x: le_dict.get(x, max(le.transform(le.classes_)) + 1))\n",
    "            data_df[col] = data_df[col].astype(np.int32)\n",
    "        else:\n",
    "            data_df[col] = scalers['numeric'][col].transform(data_df[col].values.reshape(-1, 1)).squeeze()\n",
    "            data_df[col] = data_df[col].astype(np.float32)\n",
    "            \n",
    "    # split data also by location: keep some locations for training, some for validation, and some for testing\n",
    "    # Split locations into train, validation, and test sets (60%, 20%, 20%)\n",
    "    #np.random.seed(42)  # for reproducibility\n",
    "    #unique_locations = np.random.permutation(unique_locations)\n",
    "    #num_locations = len(unique_locations)\n",
    "    #train_end = int(0.7 * num_locations)\n",
    "    #validation_end = int(0.9 * num_locations)\n",
    "\n",
    "    #train_locations = unique_locations[:train_end]\n",
    "    #validation_locations = unique_locations[train_end:validation_end]\n",
    "    #test_locations = unique_locations[validation_end:]\n",
    "\n",
    "\n",
    "    print(data_df['time'].min())\n",
    "    print(data_df['time'].max())    \n",
    "    \n",
    "    #regular splitting (not testing)\n",
    "    #train_subset_start = data_df.loc[data_df['time'] < datetime(1992, 1, 1)]\n",
    "    #train_subset_end = data_df.loc[data_df['time'] >= datetime(2011, 1, 1)]\n",
    "    #validation_subset = data_df[(data_df['time']  >= datetime(1992, 1, 1)) & (data_df['time'] < datetime(2001, 1, 1))]\n",
    "    #test_subset = data_df[(data_df['time'] >= datetime(2001, 1, 1))& (data_df['time'] < datetime(2011, 1, 1))]\n",
    "    #train_subset = pd.concat([train_subset_start, train_subset_end], axis=0)\n",
    "    \n",
    "    #40 year test\n",
    "    test_subset = data_df[(data_df['time'] >= datetime(1982, 1, 1))& (data_df['time'] < datetime(2022, 1, 1))]\n",
    "\n",
    "\n",
    "    #print(str(len(train_subset['location'].unique()))+\" train unique locations.\")\n",
    "    #print(str(len(validation_subset['location'].unique()))+\" validation unique locations.\")\n",
    "    print(str(len(test_subset['location'].unique()))+\" test unique locations.\")\n",
    "    \n",
    "    # Split data by location\n",
    "    #train_subset = train_subset[train_subset['location'].isin(train_locations)]\n",
    "    #validation_subset = validation_subset[validation_subset['location'].isin(validation_locations)]\n",
    "    #test_subset = test_subset[test_subset['location'].isin(test_locations)]\n",
    "    \n",
    "    '''subsets_dict = {'train': train_subset,\n",
    "                    'validation': validation_subset,\n",
    "                    'test': test_subset}'''\n",
    "    \n",
    "    subsets_dict = {'test': test_subset}\n",
    "\n",
    "    #print(str(len(train_subset['location'].unique()))+\" train unique locations.\")\n",
    "    #print(str(len(validation_subset['location'].unique()))+\" validation unique locations.\")\n",
    "    print(str(len(test_subset['location'].unique()))+\" test unique locations.\")\n",
    "    \n",
    "    data_sets = {'train': dict(),'validation': dict(),'test': dict()}\n",
    "    \n",
    "    for subset_name, subset_data in subsets_dict.items():\n",
    "        subset_data['id'] = subset_data['location'].astype(str) + '_' + subset_data['time'].astype(str)\n",
    "\n",
    "    for subset_key, subset_data in subsets_dict.items():\n",
    "        print(subset_key)\n",
    "        samp_interval =hist_len+fut_len\n",
    "        history_len = hist_len\n",
    "        future_len = fut_len \n",
    "        # sliding window, according to samp_interval skips between adjacent windows\n",
    "        for i in range(0, len(subset_data), samp_interval):              \n",
    "            slc = subset_data.iloc[i: i + history_len + future_len]\n",
    "            #print(i,i + history_len + future_len)\n",
    "            if len(slc) < (history_len + future_len) or slc.iloc[0]['location']!=slc.iloc[-1]['location'] or (slc.iloc[-1]['time'] - slc.iloc[0]['time'])>timedelta(days=samp_interval):\n",
    "                # skip edge cases, where not enough steps are included\n",
    "                if (slc.iloc[-1]['time'] - slc.iloc[0]['time'])>timedelta(days=samp_interval):\n",
    "                    print('SKIP starts at:', slc.iloc[0]['time'], 'ends at ',slc.iloc[-1]['time'])\n",
    "                #print('switching time series: ', slc.iloc[0]['location'], slc.iloc[-1]['location'])\n",
    "                continue\n",
    "            # meta\n",
    "            data_sets[subset_key].setdefault('time_index', []).append(slc.iloc[history_len - 1]['location'])\n",
    "            #print(slc.iloc[:history_len]['location'])\n",
    "            #print(slc.iloc[history_len:]['sif_clear_inst'])\n",
    "    \n",
    "            # static attributes\n",
    "            data_sets[subset_key].setdefault('static_feats_numeric', []).append(\n",
    "                slc.iloc[0][feature_map['static_feats_numeric']].values.astype(np.float32))\n",
    "            data_sets[subset_key].setdefault('static_feats_categorical', []).append(\n",
    "                slc.iloc[0][feature_map['static_feats_categorical']].values.astype(np.int32))\n",
    "    \n",
    "            # historical\n",
    "            data_sets[subset_key].setdefault('historical_ts_numeric', []).append(\n",
    "                slc.iloc[:history_len][feature_map['historical_ts_numeric']].values.astype(np.float32).reshape(\n",
    "                    history_len, -1))\n",
    "            data_sets[subset_key].setdefault('historical_ts_categorical', []).append(\n",
    "                slc.iloc[:history_len][feature_map['historical_ts_categorical']].values.astype(np.int32).reshape(\n",
    "                    history_len, -1))\n",
    "    \n",
    "            # futuristic (known)\n",
    "            data_sets[subset_key].setdefault('future_ts_numeric', []).append(\n",
    "                slc.iloc[history_len:][feature_map['future_ts_numeric']].values.astype(np.float32).reshape(future_len,\n",
    "                                                                                                           -1))\n",
    "            data_sets[subset_key].setdefault('future_ts_categorical', []).append(\n",
    "                slc.iloc[history_len:][feature_map['future_ts_categorical']].values.astype(np.int32).reshape(future_len,\n",
    "                                                                                                             -1))\n",
    "    \n",
    "            # target\n",
    "            data_sets[subset_key].setdefault('target', []).append(\n",
    "                slc.iloc[history_len:]['sif_clear_inst'].values.astype(np.float32))\n",
    "            data_sets[subset_key].setdefault('id', []).append(\n",
    "                slc.iloc[history_len:]['id'].values.astype(str))\n",
    "            #break\n",
    "        #break\n",
    "    # for each set\n",
    "    print('Saving...')\n",
    "    for set_key in list(data_sets.keys()):\n",
    "        # for each component in the set\n",
    "        for arr_key in list(data_sets[set_key].keys()):\n",
    "            # list of arrays will be concatenated\n",
    "            if isinstance(data_sets[set_key][arr_key], np.ndarray):\n",
    "                data_sets[set_key][arr_key] = np.stack(data_sets[set_key][arr_key], axis=0)\n",
    "            # lists will be transformed into arrays\n",
    "            else:\n",
    "                data_sets[set_key][arr_key] = np.array(data_sets[set_key][arr_key])\n",
    "\n",
    "    output_path = os.path.abspath(output_path)\n",
    "    file_name= output_filename\n",
    "    \n",
    "    #testing the output\n",
    "    #check if length of train subset time index >0 and print it \n",
    "    #print('length of train subset time index:',len(data_sets['train']['time_index']))\n",
    "    #check if length of validation subset time index >0 and print it\n",
    "    #print('length of validation subset time index:',len(data_sets['validation']['time_index']))\n",
    "    #check if length of test subset time index >0 and print it\n",
    "    print('length of test subset time index:',len(data_sets['test']['time_index']))\n",
    "    #if len test subset time index <0 pritn error message \n",
    "        \n",
    "\n",
    "    with open(os.path.join(output_path, file_name), 'wb') as f:\n",
    "            pickle.dump({\n",
    "                'data_sets': data_sets,\n",
    "                'feature_map': feature_map,\n",
    "                'scalers': scalers,\n",
    "                'categorical_cardinalities': categorical_cardinalities\n",
    "            }, f, pickle.HIGHEST_PROTOCOL)\n",
    "    print('Done!')\n",
    "    \n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    print('start')\n",
    "    \"\"\"filename = \"merged_BDT_1982_2021.parquet\"\n",
    "    data_path='/burg/glab/users/al4385/data/CSIFMETEO/'+filename\n",
    "    sorted_path= \"/burg/glab/users/al4385/data/data/sorted_TFT_30_40_BDTinterval/sorted\"+filename\n",
    "    output_path=\"/burg/glab/users/al4385/data/TFT_30_40_BDTinterval\"\n",
    "\n",
    "    hist_len=365\n",
    "    fut_len=30    \n",
    "    \n",
    "    \n",
    "    print('Checking data path:', data_path)\n",
    "    print('Checking output path:', output_path)\n",
    "    sort_loc_time(data_path, sorted_path)\n",
    "    tft_process(sorted_path, hist_len, fut_len, output_path)\"\"\"\n",
    "    \n",
    "    \n",
    "    #filename = \"sorted_BDT_-20_-60_1982_2021.parquet\"\n",
    "    #filename = \"sorted_BDT_-20_20_merged_1982_2021.parquet\"\n",
    "    #filename = \"sorted_BDT_50_20_merged_1982_2021.parquet\"\n",
    "    filename = \"sorted_BDT_50_90_1982_2021.parquet\"\n",
    "    \n",
    "    data_path='/burg/glab/users/al4385/data/CSIFMETEO/'+filename\n",
    "    #sorted_path= \"/burg/glab/users/al4385/data/data/sorted_TFT_30_40_BDTinterval/sorted\"+filename\n",
    "    sorted_path = data_path\n",
    "    output_path=\"/burg/glab/users/al4385/data/TFT_30_40_BDTinterval\"\n",
    "\n",
    "    hist_len=365\n",
    "    fut_len=30    \n",
    "    \n",
    "    \n",
    "    print('Checking data path:', data_path)\n",
    "    print('Checking output path:', output_path)\n",
    "    #sort_loc_time(data_path, sorted_path)\n",
    "    tft_process(sorted_path, hist_len, fut_len, output_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
