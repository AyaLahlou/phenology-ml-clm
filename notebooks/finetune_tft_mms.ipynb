{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da0d474f",
   "metadata": {},
   "source": [
    "# Fine-tune TFT on US MMS Data\n",
    "This notebook fine-tunes a pretrained Temporal Fusion Transformer using the US MMS dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "326736c0-29d5-481e-8a9f-10af624aff1f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wandb\n",
      "  Using cached wandb-0.20.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from wandb) (8.2.1)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from wandb) (3.1.44)\n",
      "Requirement already satisfied: packaging in /srv/conda/envs/notebook/lib/python3.12/site-packages (from wandb) (24.2)\n",
      "Requirement already satisfied: platformdirs in /srv/conda/envs/notebook/lib/python3.12/site-packages (from wandb) (4.3.8)\n",
      "Collecting protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 (from wandb)\n",
      "  Using cached protobuf-6.31.1-cp39-abi3-manylinux2014_x86_64.whl.metadata (593 bytes)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from wandb) (5.9.8)\n",
      "Requirement already satisfied: pydantic<3 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from wandb) (2.11.4)\n",
      "Requirement already satisfied: pyyaml in /srv/conda/envs/notebook/lib/python3.12/site-packages (from wandb) (6.0.2)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from wandb) (2.32.3)\n",
      "Collecting sentry-sdk>=2.0.0 (from wandb)\n",
      "  Using cached sentry_sdk-2.29.1-py2.py3-none-any.whl.metadata (10 kB)\n",
      "Collecting setproctitle (from wandb)\n",
      "  Using cached setproctitle-1.3.6-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.8 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from wandb) (4.13.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from pydantic<3->wandb) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from pydantic<3->wandb) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from pydantic<3->wandb) (0.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from requests<3,>=2.0.0->wandb) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from requests<3,>=2.0.0->wandb) (1.26.19)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from requests<3,>=2.0.0->wandb) (2025.4.26)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
      "Using cached wandb-0.20.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (23.2 MB)\n",
      "Using cached protobuf-6.31.1-cp39-abi3-manylinux2014_x86_64.whl (321 kB)\n",
      "Using cached sentry_sdk-2.29.1-py2.py3-none-any.whl (341 kB)\n",
      "Using cached setproctitle-1.3.6-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (31 kB)\n",
      "Installing collected packages: setproctitle, sentry-sdk, protobuf, wandb\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4/4\u001b[0m [wandb]32m3/4\u001b[0m [wandb]-sdk]\n",
      "Successfully installed protobuf-6.31.1 sentry-sdk-2.29.1 setproctitle-1.3.6 wandb-0.20.1\n"
     ]
    }
   ],
   "source": [
    "!pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd47f984-5cad-44c8-a3c5-90f0807bcf59",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tft-torch\n",
      "  Using cached tft_torch-0.0.6-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: omegaconf in /srv/conda/envs/notebook/lib/python3.12/site-packages (from tft-torch) (2.3.0)\n",
      "Requirement already satisfied: torch>=1.6.0 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from tft-torch) (2.5.1.post303)\n",
      "Requirement already satisfied: numpy in /srv/conda/envs/notebook/lib/python3.12/site-packages (from tft-torch) (2.2.6)\n",
      "Requirement already satisfied: pandas in /srv/conda/envs/notebook/lib/python3.12/site-packages (from tft-torch) (2.2.3)\n",
      "Requirement already satisfied: scikit-learn in /srv/conda/envs/notebook/lib/python3.12/site-packages (from tft-torch) (1.6.1)\n",
      "Requirement already satisfied: matplotlib in /srv/conda/envs/notebook/lib/python3.12/site-packages (from tft-torch) (3.10.3)\n",
      "Requirement already satisfied: filelock in /srv/conda/envs/notebook/lib/python3.12/site-packages (from torch>=1.6.0->tft-torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from torch>=1.6.0->tft-torch) (4.13.2)\n",
      "Requirement already satisfied: networkx in /srv/conda/envs/notebook/lib/python3.12/site-packages (from torch>=1.6.0->tft-torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from torch>=1.6.0->tft-torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /srv/conda/envs/notebook/lib/python3.12/site-packages (from torch>=1.6.0->tft-torch) (2025.5.0)\n",
      "Requirement already satisfied: setuptools in /srv/conda/envs/notebook/lib/python3.12/site-packages (from torch>=1.6.0->tft-torch) (80.8.0)\n",
      "Requirement already satisfied: sympy!=1.13.2,>=1.13.1 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from torch>=1.6.0->tft-torch) (1.14.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from sympy!=1.13.2,>=1.13.1->torch>=1.6.0->tft-torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from jinja2->torch>=1.6.0->tft-torch) (3.0.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from matplotlib->tft-torch) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from matplotlib->tft-torch) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from matplotlib->tft-torch) (4.58.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from matplotlib->tft-torch) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from matplotlib->tft-torch) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from matplotlib->tft-torch) (11.2.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from matplotlib->tft-torch) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from matplotlib->tft-torch) (2.9.0)\n",
      "Requirement already satisfied: six>=1.5 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib->tft-torch) (1.17.0)\n",
      "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /srv/conda/envs/notebook/lib/python3.12/site-packages (from omegaconf->tft-torch) (4.9.3)\n",
      "Requirement already satisfied: PyYAML>=5.1.0 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from omegaconf->tft-torch) (6.0.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from pandas->tft-torch) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from pandas->tft-torch) (2025.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from scikit-learn->tft-torch) (1.15.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from scikit-learn->tft-torch) (1.5.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from scikit-learn->tft-torch) (3.6.0)\n",
      "Using cached tft_torch-0.0.6-py3-none-any.whl (21 kB)\n",
      "Installing collected packages: tft-torch\n",
      "Successfully installed tft-torch-0.0.6\n"
     ]
    }
   ],
   "source": [
    "!pip install tft-torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "181cb9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import pickle\n",
    "from typing import Dict, List, Tuple\n",
    "from functools import partial\n",
    "import copy\n",
    "import numpy as np\n",
    "from omegaconf import OmegaConf, DictConfig\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch import optim\n",
    "from torch import nn\n",
    "import torch.nn.init as init\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from tft_torch.tft import TemporalFusionTransformer\n",
    "import tft_torch.loss as tft_loss\n",
    "import json\n",
    "import os \n",
    "import wandb\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03ab7679-8e7a-446a-8ab7-b2aafa60e515",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename= \"/home/jovyan/phenology-ml-clm/data/sorted_BDT_50_20_merged_1982_2021_US_MMS.pkl\"\n",
    "checkpoint= \"/home/jovyan/phenology-ml-clm/docs/weights_merged_BDT_1982_2021_feb2025_checkpoint.pth\"\n",
    "output_path= \"/home/jovyan/phenology-ml-clm/data/US_MMS_finetuned_from_feb_chkpt_060725.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e1686e5a-0c8b-43fd-8d97-16b5675f8b2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Failed to detect the name of this notebook. You can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mayalahlou\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.20.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jovyan/phenology-ml-clm/notebooks/wandb/run-20250608_031559-2erifkce</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ayalahlou/TL_US_MMS/runs/2erifkce' target=\"_blank\">curious-jazz-2</a></strong> to <a href='https://wandb.ai/ayalahlou/TL_US_MMS' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ayalahlou/TL_US_MMS' target=\"_blank\">https://wandb.ai/ayalahlou/TL_US_MMS</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ayalahlou/TL_US_MMS/runs/2erifkce' target=\"_blank\">https://wandb.ai/ayalahlou/TL_US_MMS/runs/2erifkce</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/ayalahlou/TL_US_MMS/runs/2erifkce?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7dda16ac8740>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "configuration = {'optimization':\n",
    "                 {\n",
    "                     'batch_size': {'training': 8, 'inference': 8},# both weere 64 before\n",
    "                     'learning_rate': 1e-4,#was 0.001\n",
    "                     'max_grad_norm': 1.0,\n",
    "                 }\n",
    "                 ,\n",
    "                 'model':\n",
    "                 {\n",
    "                     'dropout': 0.2,#was 0.05 before\n",
    "                     'state_size': 160,\n",
    "                     'output_quantiles': [0.1, 0.5, 0.9],\n",
    "                     'lstm_layers': 4,#was 2\n",
    "                     'attention_heads': 4 #was 4 #then 6\n",
    "                 },\n",
    "                 # these arguments are related to possible extensions of the model class\n",
    "                 'task_type':'regression',\n",
    "                 'target_window_start': None, \n",
    "                 'training_data':  filename,\n",
    "                'checkpoint': checkpoint, \n",
    "                'output_path':output_path, }\n",
    "\n",
    "wandb.init(project=\"TL_US_MMS\", config = configuration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9c59ece-6434-44e8-b284-7b8d18d69955",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(filename,'rb') as fp:\n",
    "        data = pickle.load(fp)\n",
    "    \n",
    "feature_map = data['feature_map']\n",
    "cardinalities_map = data['categorical_cardinalities']\n",
    "\n",
    "structure = {\n",
    "        'num_historical_numeric': len(feature_map['historical_ts_numeric']),\n",
    "        'num_historical_categorical': len(feature_map['historical_ts_categorical']),\n",
    "        'num_static_numeric': len(feature_map['static_feats_numeric']),\n",
    "        'num_static_categorical': len(feature_map['static_feats_categorical']),\n",
    "        'num_future_numeric': len(feature_map['future_ts_numeric']),\n",
    "        'num_future_categorical': len(feature_map['future_ts_categorical']),\n",
    "        'historical_categorical_cardinalities': [cardinalities_map[feat] + 1 for feat in feature_map['historical_ts_categorical']],\n",
    "        'static_categorical_cardinalities': [cardinalities_map[feat] + 1 for feat in feature_map['static_feats_categorical']],\n",
    "        'future_categorical_cardinalities': [cardinalities_map[feat] + 1 for feat in feature_map['future_ts_categorical']],\n",
    "    }\n",
    "\n",
    "configuration['data_props'] = structure\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83a6ec6-ae71-4938-9967-5b3b56479a52",
   "metadata": {},
   "source": [
    "# random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8eb5ceb0-eee6-4d65-8c56-33a4125c17f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "def weight_init(m):\n",
    "    \"\"\"\n",
    "    Usage:\n",
    "        model = Model()\n",
    "        model.apply(weight_init)\n",
    "    \"\"\"\n",
    "    if isinstance(m, nn.Conv1d):\n",
    "        init.normal_(m.weight.data)\n",
    "        if m.bias is not None:\n",
    "            init.normal_(m.bias.data)\n",
    "    elif isinstance(m, nn.Conv2d):\n",
    "        init.xavier_normal_(m.weight.data)\n",
    "        if m.bias is not None:\n",
    "            init.normal_(m.bias.data)\n",
    "    elif isinstance(m, nn.Conv3d):\n",
    "        init.xavier_normal_(m.weight.data)\n",
    "        if m.bias is not None:\n",
    "            init.normal_(m.bias.data)\n",
    "    elif isinstance(m, nn.ConvTranspose1d):\n",
    "        init.normal_(m.weight.data)\n",
    "        if m.bias is not None:\n",
    "            init.normal_(m.bias.data)\n",
    "    elif isinstance(m, nn.ConvTranspose2d):\n",
    "        init.xavier_normal_(m.weight.data)\n",
    "        if m.bias is not None:\n",
    "            init.normal_(m.bias.data)\n",
    "    elif isinstance(m, nn.ConvTranspose3d):\n",
    "        init.xavier_normal_(m.weight.data)\n",
    "        if m.bias is not None:\n",
    "            init.normal_(m.bias.data)\n",
    "    elif isinstance(m, nn.BatchNorm1d):\n",
    "        init.normal_(m.weight.data, mean=1, std=0.02)\n",
    "        init.constant_(m.bias.data, 0)\n",
    "    elif isinstance(m, nn.BatchNorm2d):\n",
    "        init.normal_(m.weight.data, mean=1, std=0.02)\n",
    "        init.constant_(m.bias.data, 0)\n",
    "    elif isinstance(m, nn.BatchNorm3d):\n",
    "        init.normal_(m.weight.data, mean=1, std=0.02)\n",
    "        init.constant_(m.bias.data, 0)\n",
    "    elif isinstance(m, nn.Linear):\n",
    "        init.xavier_normal_(m.weight.data)\n",
    "        if m.bias is not None:\n",
    "            init.normal_(m.bias.data)\n",
    "    elif isinstance(m, nn.LSTM):\n",
    "        for param in m.parameters():\n",
    "            if len(param.shape) >= 2:\n",
    "                init.orthogonal_(param.data)\n",
    "            else:\n",
    "                init.normal_(param.data)\n",
    "    elif isinstance(m, nn.LSTMCell):\n",
    "        for param in m.parameters():\n",
    "            if len(param.shape) >= 2:\n",
    "                init.orthogonal_(param.data)\n",
    "            else:\n",
    "                init.normal_(param.data)\n",
    "    elif isinstance(m, nn.GRU):\n",
    "        for param in m.parameters():\n",
    "            if len(param.shape) >= 2:\n",
    "                init.orthogonal_(param.data)\n",
    "            else:\n",
    "                init.normal_(param.data)\n",
    "        for names in m._all_weights:\n",
    "            for name in filter(lambda n: \"bias\" in n, names):\n",
    "                bias = getattr(m, name)\n",
    "                n = bias.size(0)\n",
    "                bias.data[:n // 3].fill_(-1.)\n",
    "    elif isinstance(m, nn.GRUCell):\n",
    "        for param in m.parameters():\n",
    "            if len(param.shape) >= 2:\n",
    "                init.orthogonal_(param.data)\n",
    "            else:\n",
    "                init.normal_(param.data)\n",
    "class DictDataSet(Dataset):\n",
    "    def __init__(self, array_dict: Dict[str, np.ndarray]):\n",
    "        self.keys_list = []\n",
    "        for k, v in array_dict.items():\n",
    "            self.keys_list.append(k)\n",
    "            if np.issubdtype(v.dtype, np.dtype('bool')):\n",
    "                setattr(self, k, torch.ByteTensor(v))\n",
    "            elif np.issubdtype(v.dtype, np.int8):\n",
    "                setattr(self, k, torch.CharTensor(v))\n",
    "            elif np.issubdtype(v.dtype, np.int16):\n",
    "                setattr(self, k, torch.ShortTensor(v))\n",
    "            elif np.issubdtype(v.dtype, np.int32):\n",
    "                setattr(self, k, torch.IntTensor(v))\n",
    "            elif np.issubdtype(v.dtype, np.int64):\n",
    "                setattr(self, k, torch.LongTensor(v))\n",
    "            elif np.issubdtype(v.dtype, np.float32):\n",
    "                setattr(self, k, torch.FloatTensor(v))\n",
    "            elif np.issubdtype(v.dtype, np.float64):\n",
    "                setattr(self, k, torch.DoubleTensor(v))\n",
    "            else:\n",
    "                setattr(self, k, torch.FloatTensor(v))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return {k: getattr(self, k)[index] for k in self.keys_list}\n",
    "\n",
    "    def __len__(self):\n",
    "        return getattr(self, self.keys_list[0]).shape[0]\n",
    "                \n",
    "                \n",
    "def recycle(iterable):\n",
    "    while True:\n",
    "        for x in iterable:\n",
    "            yield x\n",
    "\n",
    "def get_set_and_loaders(data_dict: Dict[str, np.ndarray],\n",
    "                        shuffled_loader_config: Dict,\n",
    "                        serial_loader_config: Dict,\n",
    "                        ignore_keys: List[str] = None,\n",
    "                        ) -> Tuple[torch.utils.data.Dataset, torch.utils.data.DataLoader, torch.utils.data.DataLoader]:\n",
    "    dataset = DictDataSet({k:v for k,v in data_dict.items() if (ignore_keys and k not in ignore_keys)})\n",
    "    loader = torch.utils.data.DataLoader(dataset,**shuffled_loader_config)\n",
    "    serial_loader = torch.utils.data.DataLoader(dataset,**serial_loader_config)\n",
    "\n",
    "    return dataset,iter(recycle(loader)),serial_loader\n",
    "\n",
    "class QueueAggregator(object):\n",
    "    def __init__(self, max_size):\n",
    "        self._queued_list = []\n",
    "        self.max_size = max_size\n",
    "\n",
    "    def append(self, elem):\n",
    "        self._queued_list.append(elem)\n",
    "        if len(self._queued_list) > self.max_size:\n",
    "            self._queued_list.pop(0)\n",
    "\n",
    "    def get(self):\n",
    "        return self._queued_list\n",
    "    \n",
    "class EarlyStopping(object):\n",
    "    def __init__(self, mode='min', min_delta=0, patience=10, percentage=False):\n",
    "        self.mode = mode\n",
    "        self.min_delta = min_delta\n",
    "        self.patience = patience\n",
    "        self.best = None\n",
    "        self.num_bad_epochs = 0\n",
    "        self.is_better = None\n",
    "        self._init_is_better(mode, min_delta, percentage)\n",
    "\n",
    "        if patience == 0:\n",
    "            self.is_better = lambda a, b: True\n",
    "            self.step = lambda a: False\n",
    "\n",
    "    def step(self, metrics):\n",
    "        if self.best is None:\n",
    "            self.best = metrics\n",
    "            return False\n",
    "\n",
    "        if torch.isnan(metrics):\n",
    "            return True\n",
    "\n",
    "        if self.is_better(metrics, self.best):\n",
    "            self.num_bad_epochs = 0\n",
    "            self.best = metrics\n",
    "        else:\n",
    "            self.num_bad_epochs += 1\n",
    "\n",
    "        if self.num_bad_epochs >= self.patience:\n",
    "            return True\n",
    "\n",
    "        return False\n",
    "\n",
    "    def _init_is_better(self, mode, min_delta, percentage):\n",
    "        if mode not in {'min', 'max'}:\n",
    "            raise ValueError('mode ' + mode + ' is unknown!')\n",
    "        if not percentage:\n",
    "            if mode == 'min':\n",
    "                self.is_better = lambda a, best: a < best - min_delta\n",
    "            if mode == 'max':\n",
    "                self.is_better = lambda a, best: a > best + min_delta\n",
    "        else:\n",
    "            if mode == 'min':\n",
    "                self.is_better = lambda a, best: a < best - (\n",
    "                            best * min_delta / 100)\n",
    "            if mode == 'max':\n",
    "                self.is_better = lambda a, best: a > best + (\n",
    "                            best * min_delta / 100)\n",
    "                \n",
    "def process_batch(batch: Dict[str,torch.tensor],\n",
    "                  model: nn.Module,\n",
    "                  quantiles_tensor: torch.tensor,\n",
    "                  device:torch.device):\n",
    "    if is_cuda:\n",
    "        for k in list(batch.keys()):\n",
    "            batch[k] = batch[k].to(device)\n",
    "\n",
    "    batch_outputs = model(batch)\n",
    "    labels = batch['target']\n",
    "\n",
    "    predicted_quantiles = batch_outputs['predicted_quantiles']\n",
    "    q_loss, q_risk, _ = tft_loss.get_quantiles_loss_and_q_risk(outputs=predicted_quantiles,\n",
    "                                                              targets=labels,\n",
    "                                                              desired_quantiles=quantiles_tensor)\n",
    "    return q_loss, q_risk\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c42042-36a6-47b3-99b5-c05c1f88b970",
   "metadata": {},
   "source": [
    "# Load model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a997eb5-1018-4f61-ba3a-93084b04ea70",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TemporalFusionTransformer(config=OmegaConf.create(configuration))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9b3da8d1-e884-4dbd-a1c0-b6e99aa3d80b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TemporalFusionTransformer(\n",
       "  (static_transform): InputChannelEmbedding(\n",
       "    (numeric_transform): NumericInputTransformation(\n",
       "      (numeric_projection_layers): ModuleList(\n",
       "        (0-1): 2 x Linear(in_features=1, out_features=160, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (categorical_transform): NullTransform()\n",
       "  )\n",
       "  (historical_ts_transform): InputChannelEmbedding(\n",
       "    (numeric_transform): TimeDistributed(\n",
       "      (module): NumericInputTransformation(\n",
       "        (numeric_projection_layers): ModuleList(\n",
       "          (0-6): 7 x Linear(in_features=1, out_features=160, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (categorical_transform): NullTransform()\n",
       "  )\n",
       "  (future_ts_transform): InputChannelEmbedding(\n",
       "    (numeric_transform): TimeDistributed(\n",
       "      (module): NumericInputTransformation(\n",
       "        (numeric_projection_layers): ModuleList(\n",
       "          (0-5): 6 x Linear(in_features=1, out_features=160, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (categorical_transform): NullTransform()\n",
       "  )\n",
       "  (static_selection): VariableSelectionNetwork(\n",
       "    (flattened_grn): GatedResidualNetwork(\n",
       "      (skip_layer): TimeDistributed(\n",
       "        (module): Linear(in_features=320, out_features=2, bias=True)\n",
       "      )\n",
       "      (fc1): TimeDistributed(\n",
       "        (module): Linear(in_features=320, out_features=160, bias=True)\n",
       "      )\n",
       "      (elu1): ELU(alpha=1.0)\n",
       "      (fc2): TimeDistributed(\n",
       "        (module): Linear(in_features=160, out_features=2, bias=True)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.2, inplace=False)\n",
       "      (gate): TimeDistributed(\n",
       "        (module): GatedLinearUnit(\n",
       "          (fc1): Linear(in_features=2, out_features=2, bias=True)\n",
       "          (fc2): Linear(in_features=2, out_features=2, bias=True)\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "      (layernorm): TimeDistributed(\n",
       "        (module): LayerNorm((2,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (softmax): Softmax(dim=1)\n",
       "    (single_variable_grns): ModuleList(\n",
       "      (0-1): 2 x GatedResidualNetwork(\n",
       "        (fc1): TimeDistributed(\n",
       "          (module): Linear(in_features=160, out_features=160, bias=True)\n",
       "        )\n",
       "        (elu1): ELU(alpha=1.0)\n",
       "        (fc2): TimeDistributed(\n",
       "          (module): Linear(in_features=160, out_features=160, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "        (gate): TimeDistributed(\n",
       "          (module): GatedLinearUnit(\n",
       "            (fc1): Linear(in_features=160, out_features=160, bias=True)\n",
       "            (fc2): Linear(in_features=160, out_features=160, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (layernorm): TimeDistributed(\n",
       "          (module): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (historical_ts_selection): VariableSelectionNetwork(\n",
       "    (flattened_grn): GatedResidualNetwork(\n",
       "      (skip_layer): TimeDistributed(\n",
       "        (module): Linear(in_features=1120, out_features=7, bias=True)\n",
       "      )\n",
       "      (fc1): TimeDistributed(\n",
       "        (module): Linear(in_features=1120, out_features=160, bias=True)\n",
       "      )\n",
       "      (context_projection): TimeDistributed(\n",
       "        (module): Linear(in_features=160, out_features=160, bias=False)\n",
       "      )\n",
       "      (elu1): ELU(alpha=1.0)\n",
       "      (fc2): TimeDistributed(\n",
       "        (module): Linear(in_features=160, out_features=7, bias=True)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.2, inplace=False)\n",
       "      (gate): TimeDistributed(\n",
       "        (module): GatedLinearUnit(\n",
       "          (fc1): Linear(in_features=7, out_features=7, bias=True)\n",
       "          (fc2): Linear(in_features=7, out_features=7, bias=True)\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "      (layernorm): TimeDistributed(\n",
       "        (module): LayerNorm((7,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (softmax): Softmax(dim=1)\n",
       "    (single_variable_grns): ModuleList(\n",
       "      (0-6): 7 x GatedResidualNetwork(\n",
       "        (fc1): TimeDistributed(\n",
       "          (module): Linear(in_features=160, out_features=160, bias=True)\n",
       "        )\n",
       "        (elu1): ELU(alpha=1.0)\n",
       "        (fc2): TimeDistributed(\n",
       "          (module): Linear(in_features=160, out_features=160, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "        (gate): TimeDistributed(\n",
       "          (module): GatedLinearUnit(\n",
       "            (fc1): Linear(in_features=160, out_features=160, bias=True)\n",
       "            (fc2): Linear(in_features=160, out_features=160, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (layernorm): TimeDistributed(\n",
       "          (module): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (future_ts_selection): VariableSelectionNetwork(\n",
       "    (flattened_grn): GatedResidualNetwork(\n",
       "      (skip_layer): TimeDistributed(\n",
       "        (module): Linear(in_features=960, out_features=6, bias=True)\n",
       "      )\n",
       "      (fc1): TimeDistributed(\n",
       "        (module): Linear(in_features=960, out_features=160, bias=True)\n",
       "      )\n",
       "      (context_projection): TimeDistributed(\n",
       "        (module): Linear(in_features=160, out_features=160, bias=False)\n",
       "      )\n",
       "      (elu1): ELU(alpha=1.0)\n",
       "      (fc2): TimeDistributed(\n",
       "        (module): Linear(in_features=160, out_features=6, bias=True)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.2, inplace=False)\n",
       "      (gate): TimeDistributed(\n",
       "        (module): GatedLinearUnit(\n",
       "          (fc1): Linear(in_features=6, out_features=6, bias=True)\n",
       "          (fc2): Linear(in_features=6, out_features=6, bias=True)\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "      (layernorm): TimeDistributed(\n",
       "        (module): LayerNorm((6,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (softmax): Softmax(dim=1)\n",
       "    (single_variable_grns): ModuleList(\n",
       "      (0-5): 6 x GatedResidualNetwork(\n",
       "        (fc1): TimeDistributed(\n",
       "          (module): Linear(in_features=160, out_features=160, bias=True)\n",
       "        )\n",
       "        (elu1): ELU(alpha=1.0)\n",
       "        (fc2): TimeDistributed(\n",
       "          (module): Linear(in_features=160, out_features=160, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "        (gate): TimeDistributed(\n",
       "          (module): GatedLinearUnit(\n",
       "            (fc1): Linear(in_features=160, out_features=160, bias=True)\n",
       "            (fc2): Linear(in_features=160, out_features=160, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (layernorm): TimeDistributed(\n",
       "          (module): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (static_encoder_selection): GatedResidualNetwork(\n",
       "    (fc1): TimeDistributed(\n",
       "      (module): Linear(in_features=160, out_features=160, bias=True)\n",
       "    )\n",
       "    (elu1): ELU(alpha=1.0)\n",
       "    (fc2): TimeDistributed(\n",
       "      (module): Linear(in_features=160, out_features=160, bias=True)\n",
       "    )\n",
       "    (dropout): Dropout(p=0.2, inplace=False)\n",
       "    (gate): TimeDistributed(\n",
       "      (module): GatedLinearUnit(\n",
       "        (fc1): Linear(in_features=160, out_features=160, bias=True)\n",
       "        (fc2): Linear(in_features=160, out_features=160, bias=True)\n",
       "        (sigmoid): Sigmoid()\n",
       "      )\n",
       "    )\n",
       "    (layernorm): TimeDistributed(\n",
       "      (module): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (static_encoder_enrichment): GatedResidualNetwork(\n",
       "    (fc1): TimeDistributed(\n",
       "      (module): Linear(in_features=160, out_features=160, bias=True)\n",
       "    )\n",
       "    (elu1): ELU(alpha=1.0)\n",
       "    (fc2): TimeDistributed(\n",
       "      (module): Linear(in_features=160, out_features=160, bias=True)\n",
       "    )\n",
       "    (dropout): Dropout(p=0.2, inplace=False)\n",
       "    (gate): TimeDistributed(\n",
       "      (module): GatedLinearUnit(\n",
       "        (fc1): Linear(in_features=160, out_features=160, bias=True)\n",
       "        (fc2): Linear(in_features=160, out_features=160, bias=True)\n",
       "        (sigmoid): Sigmoid()\n",
       "      )\n",
       "    )\n",
       "    (layernorm): TimeDistributed(\n",
       "      (module): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (static_encoder_sequential_cell_init): GatedResidualNetwork(\n",
       "    (fc1): TimeDistributed(\n",
       "      (module): Linear(in_features=160, out_features=160, bias=True)\n",
       "    )\n",
       "    (elu1): ELU(alpha=1.0)\n",
       "    (fc2): TimeDistributed(\n",
       "      (module): Linear(in_features=160, out_features=160, bias=True)\n",
       "    )\n",
       "    (dropout): Dropout(p=0.2, inplace=False)\n",
       "    (gate): TimeDistributed(\n",
       "      (module): GatedLinearUnit(\n",
       "        (fc1): Linear(in_features=160, out_features=160, bias=True)\n",
       "        (fc2): Linear(in_features=160, out_features=160, bias=True)\n",
       "        (sigmoid): Sigmoid()\n",
       "      )\n",
       "    )\n",
       "    (layernorm): TimeDistributed(\n",
       "      (module): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (static_encoder_sequential_state_init): GatedResidualNetwork(\n",
       "    (fc1): TimeDistributed(\n",
       "      (module): Linear(in_features=160, out_features=160, bias=True)\n",
       "    )\n",
       "    (elu1): ELU(alpha=1.0)\n",
       "    (fc2): TimeDistributed(\n",
       "      (module): Linear(in_features=160, out_features=160, bias=True)\n",
       "    )\n",
       "    (dropout): Dropout(p=0.2, inplace=False)\n",
       "    (gate): TimeDistributed(\n",
       "      (module): GatedLinearUnit(\n",
       "        (fc1): Linear(in_features=160, out_features=160, bias=True)\n",
       "        (fc2): Linear(in_features=160, out_features=160, bias=True)\n",
       "        (sigmoid): Sigmoid()\n",
       "      )\n",
       "    )\n",
       "    (layernorm): TimeDistributed(\n",
       "      (module): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (past_lstm): LSTM(160, 160, num_layers=4, batch_first=True, dropout=0.2)\n",
       "  (future_lstm): LSTM(160, 160, num_layers=4, batch_first=True, dropout=0.2)\n",
       "  (post_lstm_gating): GateAddNorm(\n",
       "    (dropout_layer): Dropout(p=0.2, inplace=False)\n",
       "    (gate): TimeDistributed(\n",
       "      (module): GatedLinearUnit(\n",
       "        (fc1): Linear(in_features=160, out_features=160, bias=True)\n",
       "        (fc2): Linear(in_features=160, out_features=160, bias=True)\n",
       "        (sigmoid): Sigmoid()\n",
       "      )\n",
       "    )\n",
       "    (layernorm): TimeDistributed(\n",
       "      (module): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (static_enrichment_grn): GatedResidualNetwork(\n",
       "    (fc1): TimeDistributed(\n",
       "      (module): Linear(in_features=160, out_features=160, bias=True)\n",
       "    )\n",
       "    (context_projection): TimeDistributed(\n",
       "      (module): Linear(in_features=160, out_features=160, bias=False)\n",
       "    )\n",
       "    (elu1): ELU(alpha=1.0)\n",
       "    (fc2): TimeDistributed(\n",
       "      (module): Linear(in_features=160, out_features=160, bias=True)\n",
       "    )\n",
       "    (dropout): Dropout(p=0.2, inplace=False)\n",
       "    (gate): TimeDistributed(\n",
       "      (module): GatedLinearUnit(\n",
       "        (fc1): Linear(in_features=160, out_features=160, bias=True)\n",
       "        (fc2): Linear(in_features=160, out_features=160, bias=True)\n",
       "        (sigmoid): Sigmoid()\n",
       "      )\n",
       "    )\n",
       "    (layernorm): TimeDistributed(\n",
       "      (module): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (multihead_attn): InterpretableMultiHeadAttention(\n",
       "    (w_q): Linear(in_features=160, out_features=640, bias=True)\n",
       "    (w_k): Linear(in_features=160, out_features=640, bias=True)\n",
       "    (w_v): Linear(in_features=160, out_features=160, bias=True)\n",
       "    (out): Linear(in_features=160, out_features=160, bias=True)\n",
       "  )\n",
       "  (post_attention_gating): GateAddNorm(\n",
       "    (dropout_layer): Dropout(p=0.2, inplace=False)\n",
       "    (gate): TimeDistributed(\n",
       "      (module): GatedLinearUnit(\n",
       "        (fc1): Linear(in_features=160, out_features=160, bias=True)\n",
       "        (fc2): Linear(in_features=160, out_features=160, bias=True)\n",
       "        (sigmoid): Sigmoid()\n",
       "      )\n",
       "    )\n",
       "    (layernorm): TimeDistributed(\n",
       "      (module): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (pos_wise_ff_grn): GatedResidualNetwork(\n",
       "    (fc1): TimeDistributed(\n",
       "      (module): Linear(in_features=160, out_features=160, bias=True)\n",
       "    )\n",
       "    (elu1): ELU(alpha=1.0)\n",
       "    (fc2): TimeDistributed(\n",
       "      (module): Linear(in_features=160, out_features=160, bias=True)\n",
       "    )\n",
       "    (dropout): Dropout(p=0.2, inplace=False)\n",
       "    (gate): TimeDistributed(\n",
       "      (module): GatedLinearUnit(\n",
       "        (fc1): Linear(in_features=160, out_features=160, bias=True)\n",
       "        (fc2): Linear(in_features=160, out_features=160, bias=True)\n",
       "        (sigmoid): Sigmoid()\n",
       "      )\n",
       "    )\n",
       "    (layernorm): TimeDistributed(\n",
       "      (module): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (pos_wise_ff_gating): GateAddNorm(\n",
       "    (gate): TimeDistributed(\n",
       "      (module): GatedLinearUnit(\n",
       "        (fc1): Linear(in_features=160, out_features=160, bias=True)\n",
       "        (fc2): Linear(in_features=160, out_features=160, bias=True)\n",
       "        (sigmoid): Sigmoid()\n",
       "      )\n",
       "    )\n",
       "    (layernorm): TimeDistributed(\n",
       "      (module): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (output_layer): Linear(in_features=160, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if is_cuda else \"cpu\")\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f39c354a-f231-4758-87df-be328706cc52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_64/1604581757.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(checkpoint, map_location=device)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=['static_selection.flattened_grn.skip_layer.module.weight', 'static_selection.flattened_grn.skip_layer.module.bias', 'static_selection.flattened_grn.fc1.module.weight', 'static_selection.flattened_grn.fc2.module.weight', 'static_selection.flattened_grn.fc2.module.bias', 'static_selection.flattened_grn.gate.module.fc1.weight', 'static_selection.flattened_grn.gate.module.fc1.bias', 'static_selection.flattened_grn.gate.module.fc2.weight', 'static_selection.flattened_grn.gate.module.fc2.bias', 'static_selection.flattened_grn.layernorm.module.weight', 'static_selection.flattened_grn.layernorm.module.bias'], unexpected_keys=[])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict = torch.load(checkpoint, map_location=device)\n",
    "model_state = model.state_dict()\n",
    "\n",
    "# Filter out incompatible keys\n",
    "filtered_state_dict = {\n",
    "    k: v for k, v in state_dict.items() if k in model_state and model_state[k].shape == v.shape\n",
    "}\n",
    "\n",
    "model.load_state_dict(filtered_state_dict, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7489703c-ed82-46c4-ad6d-52cf855ebce1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TemporalFusionTransformer(\n",
       "  (static_transform): InputChannelEmbedding(\n",
       "    (numeric_transform): NumericInputTransformation(\n",
       "      (numeric_projection_layers): ModuleList(\n",
       "        (0-1): 2 x Linear(in_features=1, out_features=160, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (categorical_transform): NullTransform()\n",
       "  )\n",
       "  (historical_ts_transform): InputChannelEmbedding(\n",
       "    (numeric_transform): TimeDistributed(\n",
       "      (module): NumericInputTransformation(\n",
       "        (numeric_projection_layers): ModuleList(\n",
       "          (0-6): 7 x Linear(in_features=1, out_features=160, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (categorical_transform): NullTransform()\n",
       "  )\n",
       "  (future_ts_transform): InputChannelEmbedding(\n",
       "    (numeric_transform): TimeDistributed(\n",
       "      (module): NumericInputTransformation(\n",
       "        (numeric_projection_layers): ModuleList(\n",
       "          (0-5): 6 x Linear(in_features=1, out_features=160, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (categorical_transform): NullTransform()\n",
       "  )\n",
       "  (static_selection): VariableSelectionNetwork(\n",
       "    (flattened_grn): GatedResidualNetwork(\n",
       "      (skip_layer): TimeDistributed(\n",
       "        (module): Linear(in_features=320, out_features=2, bias=True)\n",
       "      )\n",
       "      (fc1): TimeDistributed(\n",
       "        (module): Linear(in_features=320, out_features=160, bias=True)\n",
       "      )\n",
       "      (elu1): ELU(alpha=1.0)\n",
       "      (fc2): TimeDistributed(\n",
       "        (module): Linear(in_features=160, out_features=2, bias=True)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.2, inplace=False)\n",
       "      (gate): TimeDistributed(\n",
       "        (module): GatedLinearUnit(\n",
       "          (fc1): Linear(in_features=2, out_features=2, bias=True)\n",
       "          (fc2): Linear(in_features=2, out_features=2, bias=True)\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "      (layernorm): TimeDistributed(\n",
       "        (module): LayerNorm((2,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (softmax): Softmax(dim=1)\n",
       "    (single_variable_grns): ModuleList(\n",
       "      (0-1): 2 x GatedResidualNetwork(\n",
       "        (fc1): TimeDistributed(\n",
       "          (module): Linear(in_features=160, out_features=160, bias=True)\n",
       "        )\n",
       "        (elu1): ELU(alpha=1.0)\n",
       "        (fc2): TimeDistributed(\n",
       "          (module): Linear(in_features=160, out_features=160, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "        (gate): TimeDistributed(\n",
       "          (module): GatedLinearUnit(\n",
       "            (fc1): Linear(in_features=160, out_features=160, bias=True)\n",
       "            (fc2): Linear(in_features=160, out_features=160, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (layernorm): TimeDistributed(\n",
       "          (module): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (historical_ts_selection): VariableSelectionNetwork(\n",
       "    (flattened_grn): GatedResidualNetwork(\n",
       "      (skip_layer): TimeDistributed(\n",
       "        (module): Linear(in_features=1120, out_features=7, bias=True)\n",
       "      )\n",
       "      (fc1): TimeDistributed(\n",
       "        (module): Linear(in_features=1120, out_features=160, bias=True)\n",
       "      )\n",
       "      (context_projection): TimeDistributed(\n",
       "        (module): Linear(in_features=160, out_features=160, bias=False)\n",
       "      )\n",
       "      (elu1): ELU(alpha=1.0)\n",
       "      (fc2): TimeDistributed(\n",
       "        (module): Linear(in_features=160, out_features=7, bias=True)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.2, inplace=False)\n",
       "      (gate): TimeDistributed(\n",
       "        (module): GatedLinearUnit(\n",
       "          (fc1): Linear(in_features=7, out_features=7, bias=True)\n",
       "          (fc2): Linear(in_features=7, out_features=7, bias=True)\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "      (layernorm): TimeDistributed(\n",
       "        (module): LayerNorm((7,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (softmax): Softmax(dim=1)\n",
       "    (single_variable_grns): ModuleList(\n",
       "      (0-6): 7 x GatedResidualNetwork(\n",
       "        (fc1): TimeDistributed(\n",
       "          (module): Linear(in_features=160, out_features=160, bias=True)\n",
       "        )\n",
       "        (elu1): ELU(alpha=1.0)\n",
       "        (fc2): TimeDistributed(\n",
       "          (module): Linear(in_features=160, out_features=160, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "        (gate): TimeDistributed(\n",
       "          (module): GatedLinearUnit(\n",
       "            (fc1): Linear(in_features=160, out_features=160, bias=True)\n",
       "            (fc2): Linear(in_features=160, out_features=160, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (layernorm): TimeDistributed(\n",
       "          (module): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (future_ts_selection): VariableSelectionNetwork(\n",
       "    (flattened_grn): GatedResidualNetwork(\n",
       "      (skip_layer): TimeDistributed(\n",
       "        (module): Linear(in_features=960, out_features=6, bias=True)\n",
       "      )\n",
       "      (fc1): TimeDistributed(\n",
       "        (module): Linear(in_features=960, out_features=160, bias=True)\n",
       "      )\n",
       "      (context_projection): TimeDistributed(\n",
       "        (module): Linear(in_features=160, out_features=160, bias=False)\n",
       "      )\n",
       "      (elu1): ELU(alpha=1.0)\n",
       "      (fc2): TimeDistributed(\n",
       "        (module): Linear(in_features=160, out_features=6, bias=True)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.2, inplace=False)\n",
       "      (gate): TimeDistributed(\n",
       "        (module): GatedLinearUnit(\n",
       "          (fc1): Linear(in_features=6, out_features=6, bias=True)\n",
       "          (fc2): Linear(in_features=6, out_features=6, bias=True)\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "      (layernorm): TimeDistributed(\n",
       "        (module): LayerNorm((6,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (softmax): Softmax(dim=1)\n",
       "    (single_variable_grns): ModuleList(\n",
       "      (0-5): 6 x GatedResidualNetwork(\n",
       "        (fc1): TimeDistributed(\n",
       "          (module): Linear(in_features=160, out_features=160, bias=True)\n",
       "        )\n",
       "        (elu1): ELU(alpha=1.0)\n",
       "        (fc2): TimeDistributed(\n",
       "          (module): Linear(in_features=160, out_features=160, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "        (gate): TimeDistributed(\n",
       "          (module): GatedLinearUnit(\n",
       "            (fc1): Linear(in_features=160, out_features=160, bias=True)\n",
       "            (fc2): Linear(in_features=160, out_features=160, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (layernorm): TimeDistributed(\n",
       "          (module): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (static_encoder_selection): GatedResidualNetwork(\n",
       "    (fc1): TimeDistributed(\n",
       "      (module): Linear(in_features=160, out_features=160, bias=True)\n",
       "    )\n",
       "    (elu1): ELU(alpha=1.0)\n",
       "    (fc2): TimeDistributed(\n",
       "      (module): Linear(in_features=160, out_features=160, bias=True)\n",
       "    )\n",
       "    (dropout): Dropout(p=0.2, inplace=False)\n",
       "    (gate): TimeDistributed(\n",
       "      (module): GatedLinearUnit(\n",
       "        (fc1): Linear(in_features=160, out_features=160, bias=True)\n",
       "        (fc2): Linear(in_features=160, out_features=160, bias=True)\n",
       "        (sigmoid): Sigmoid()\n",
       "      )\n",
       "    )\n",
       "    (layernorm): TimeDistributed(\n",
       "      (module): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (static_encoder_enrichment): GatedResidualNetwork(\n",
       "    (fc1): TimeDistributed(\n",
       "      (module): Linear(in_features=160, out_features=160, bias=True)\n",
       "    )\n",
       "    (elu1): ELU(alpha=1.0)\n",
       "    (fc2): TimeDistributed(\n",
       "      (module): Linear(in_features=160, out_features=160, bias=True)\n",
       "    )\n",
       "    (dropout): Dropout(p=0.2, inplace=False)\n",
       "    (gate): TimeDistributed(\n",
       "      (module): GatedLinearUnit(\n",
       "        (fc1): Linear(in_features=160, out_features=160, bias=True)\n",
       "        (fc2): Linear(in_features=160, out_features=160, bias=True)\n",
       "        (sigmoid): Sigmoid()\n",
       "      )\n",
       "    )\n",
       "    (layernorm): TimeDistributed(\n",
       "      (module): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (static_encoder_sequential_cell_init): GatedResidualNetwork(\n",
       "    (fc1): TimeDistributed(\n",
       "      (module): Linear(in_features=160, out_features=160, bias=True)\n",
       "    )\n",
       "    (elu1): ELU(alpha=1.0)\n",
       "    (fc2): TimeDistributed(\n",
       "      (module): Linear(in_features=160, out_features=160, bias=True)\n",
       "    )\n",
       "    (dropout): Dropout(p=0.2, inplace=False)\n",
       "    (gate): TimeDistributed(\n",
       "      (module): GatedLinearUnit(\n",
       "        (fc1): Linear(in_features=160, out_features=160, bias=True)\n",
       "        (fc2): Linear(in_features=160, out_features=160, bias=True)\n",
       "        (sigmoid): Sigmoid()\n",
       "      )\n",
       "    )\n",
       "    (layernorm): TimeDistributed(\n",
       "      (module): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (static_encoder_sequential_state_init): GatedResidualNetwork(\n",
       "    (fc1): TimeDistributed(\n",
       "      (module): Linear(in_features=160, out_features=160, bias=True)\n",
       "    )\n",
       "    (elu1): ELU(alpha=1.0)\n",
       "    (fc2): TimeDistributed(\n",
       "      (module): Linear(in_features=160, out_features=160, bias=True)\n",
       "    )\n",
       "    (dropout): Dropout(p=0.2, inplace=False)\n",
       "    (gate): TimeDistributed(\n",
       "      (module): GatedLinearUnit(\n",
       "        (fc1): Linear(in_features=160, out_features=160, bias=True)\n",
       "        (fc2): Linear(in_features=160, out_features=160, bias=True)\n",
       "        (sigmoid): Sigmoid()\n",
       "      )\n",
       "    )\n",
       "    (layernorm): TimeDistributed(\n",
       "      (module): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (past_lstm): LSTM(160, 160, num_layers=4, batch_first=True, dropout=0.2)\n",
       "  (future_lstm): LSTM(160, 160, num_layers=4, batch_first=True, dropout=0.2)\n",
       "  (post_lstm_gating): GateAddNorm(\n",
       "    (dropout_layer): Dropout(p=0.2, inplace=False)\n",
       "    (gate): TimeDistributed(\n",
       "      (module): GatedLinearUnit(\n",
       "        (fc1): Linear(in_features=160, out_features=160, bias=True)\n",
       "        (fc2): Linear(in_features=160, out_features=160, bias=True)\n",
       "        (sigmoid): Sigmoid()\n",
       "      )\n",
       "    )\n",
       "    (layernorm): TimeDistributed(\n",
       "      (module): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (static_enrichment_grn): GatedResidualNetwork(\n",
       "    (fc1): TimeDistributed(\n",
       "      (module): Linear(in_features=160, out_features=160, bias=True)\n",
       "    )\n",
       "    (context_projection): TimeDistributed(\n",
       "      (module): Linear(in_features=160, out_features=160, bias=False)\n",
       "    )\n",
       "    (elu1): ELU(alpha=1.0)\n",
       "    (fc2): TimeDistributed(\n",
       "      (module): Linear(in_features=160, out_features=160, bias=True)\n",
       "    )\n",
       "    (dropout): Dropout(p=0.2, inplace=False)\n",
       "    (gate): TimeDistributed(\n",
       "      (module): GatedLinearUnit(\n",
       "        (fc1): Linear(in_features=160, out_features=160, bias=True)\n",
       "        (fc2): Linear(in_features=160, out_features=160, bias=True)\n",
       "        (sigmoid): Sigmoid()\n",
       "      )\n",
       "    )\n",
       "    (layernorm): TimeDistributed(\n",
       "      (module): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (multihead_attn): InterpretableMultiHeadAttention(\n",
       "    (w_q): Linear(in_features=160, out_features=640, bias=True)\n",
       "    (w_k): Linear(in_features=160, out_features=640, bias=True)\n",
       "    (w_v): Linear(in_features=160, out_features=160, bias=True)\n",
       "    (out): Linear(in_features=160, out_features=160, bias=True)\n",
       "  )\n",
       "  (post_attention_gating): GateAddNorm(\n",
       "    (dropout_layer): Dropout(p=0.2, inplace=False)\n",
       "    (gate): TimeDistributed(\n",
       "      (module): GatedLinearUnit(\n",
       "        (fc1): Linear(in_features=160, out_features=160, bias=True)\n",
       "        (fc2): Linear(in_features=160, out_features=160, bias=True)\n",
       "        (sigmoid): Sigmoid()\n",
       "      )\n",
       "    )\n",
       "    (layernorm): TimeDistributed(\n",
       "      (module): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (pos_wise_ff_grn): GatedResidualNetwork(\n",
       "    (fc1): TimeDistributed(\n",
       "      (module): Linear(in_features=160, out_features=160, bias=True)\n",
       "    )\n",
       "    (elu1): ELU(alpha=1.0)\n",
       "    (fc2): TimeDistributed(\n",
       "      (module): Linear(in_features=160, out_features=160, bias=True)\n",
       "    )\n",
       "    (dropout): Dropout(p=0.2, inplace=False)\n",
       "    (gate): TimeDistributed(\n",
       "      (module): GatedLinearUnit(\n",
       "        (fc1): Linear(in_features=160, out_features=160, bias=True)\n",
       "        (fc2): Linear(in_features=160, out_features=160, bias=True)\n",
       "        (sigmoid): Sigmoid()\n",
       "      )\n",
       "    )\n",
       "    (layernorm): TimeDistributed(\n",
       "      (module): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (pos_wise_ff_gating): GateAddNorm(\n",
       "    (gate): TimeDistributed(\n",
       "      (module): GatedLinearUnit(\n",
       "        (fc1): Linear(in_features=160, out_features=160, bias=True)\n",
       "        (fc2): Linear(in_features=160, out_features=160, bias=True)\n",
       "        (sigmoid): Sigmoid()\n",
       "      )\n",
       "    )\n",
       "    (layernorm): TimeDistributed(\n",
       "      (module): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (output_layer): Linear(in_features=160, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f6be526b-9f19-44cd-9f01-b14ba8f5a488",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-4)\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f38fc096-dd13-4e0e-aa31-2ae16497fb78",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = optim.Adam(filter(lambda p: p.requires_grad, list(model.parameters())),\n",
    "                    lr=configuration['optimization']['learning_rate'],\n",
    "                    weight_decay=1e-4)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer=opt, mode='min', factor = 0.5, patience=5, min_lr= 1e-6 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "111de6b2-3938-41ba-ac8b-880ad832a0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_loader_config = {'batch_size': configuration['optimization']['batch_size']['training'],\n",
    "                    'drop_last': True,\n",
    "                    'shuffle':False}\n",
    "\n",
    "serial_loader_config = {'batch_size': configuration['optimization']['batch_size']['inference'],\n",
    "                'drop_last': False,\n",
    "                'shuffle':False}\n",
    "\n",
    "# the following fields do not contain actual data, but are only identifiers of each observation\n",
    "#meta_keys = ['time', 'location','soil', \"soil_x\", \"soil_y\", \"id\"]\n",
    "meta_keys = ['time', 'location', \"soil_x\", \"soil_y\", \"id\"]\n",
    "train_set,train_loader,train_serial_loader = get_set_and_loaders(data['data_sets']['train'],\n",
    "                                                                shuffled_loader_config,\n",
    "                                                                serial_loader_config,\n",
    "                                                                ignore_keys=meta_keys)\n",
    "validation_set,validation_loader,validation_serial_loader = get_set_and_loaders(data['data_sets']['validation'],\n",
    "                                                                shuffled_loader_config,\n",
    "                                                                serial_loader_config,\n",
    "                                                                ignore_keys=meta_keys)\n",
    "test_set,test_loader,test_serial_loader = get_set_and_loaders(data['data_sets']['test'],\n",
    "                                                                serial_loader_config,\n",
    "                                                                serial_loader_config,\n",
    "                                                                ignore_keys=meta_keys)\n",
    "\n",
    "# If early stopping is not triggered, after how many epochs should we quit training\n",
    "max_epochs = 100\n",
    "# how many training batches will compose a single training epoch\n",
    "epoch_iters = len(data['data_sets']['train']['time_index'])//8#was 200 #then 400\n",
    "# upon completing a training epoch, we perform an evaluation of all the subsets\n",
    "# eval_iters will define how many batches of each set will compose a single evaluation round\n",
    "eval_iters = len(data['data_sets']['validation']['time_index'])//8 #500 #then 100\n",
    "# during training, on what frequency should we display the monitored performance\n",
    "log_interval = 50\n",
    "# what is the running-window used by our QueueAggregator object for monitoring the training performance\n",
    "ma_queue_size = 50\n",
    "# how many evaluation rounds should we allow,\n",
    "# without any improvement in the performance observed on the validation set\n",
    "patience = 10\n",
    "    \n",
    "# initialize early stopping mechanism\n",
    "es = EarlyStopping(patience=patience)\n",
    "# initialize the loss aggregator for running window performance estimation\n",
    "loss_aggregator = QueueAggregator(max_size=ma_queue_size)\n",
    "\n",
    "# initialize counters\n",
    "batch_idx = 0\n",
    "epoch_idx = 0\n",
    "\n",
    "quantiles_tensor = torch.tensor(configuration['model']['output_quantiles']).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "aecc5b60-ab54-48ad-a336-ecc96a165209",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Epoch Index 0\n",
      "Evaluating train set\n",
      "Epoch: 0, Batch Index: 77- Eval train - q_loss = 0.03762 , q_risk_0.1 = 0.01942 , q_risk_0.5 = 0.04138 , q_risk_0.9 = 0.02063\n",
      "Evaluating validation set\n",
      "Epoch: 0, Batch Index: 77- Eval validation - q_loss = 0.03371 , q_risk_0.1 = 0.01723 , q_risk_0.5 = 0.03752 , q_risk_0.9 = 0.02010\n",
      "Evaluating test set\n",
      "Epoch: 0, Batch Index: 77- Eval test - q_loss = 0.03190 , q_risk_0.1 = 0.01803 , q_risk_0.5 = 0.03425 , q_risk_0.9 = 0.01925\n",
      "Epoch: 0, Batch Index: 100 - Train Loss = 0.042504263184964655\n",
      "Epoch: 0, Batch Index: 150 - Train Loss = 0.030155572667717935\n",
      "Starting Epoch Index 1\n",
      "Evaluating train set\n",
      "Epoch: 1, Batch Index: 154- Eval train - q_loss = 0.02079 , q_risk_0.1 = 0.00957 , q_risk_0.5 = 0.02376 , q_risk_0.9 = 0.01354\n",
      "Evaluating validation set\n",
      "Epoch: 1, Batch Index: 154- Eval validation - q_loss = 0.02428 , q_risk_0.1 = 0.01234 , q_risk_0.5 = 0.02923 , q_risk_0.9 = 0.01397\n",
      "Evaluating test set\n",
      "Epoch: 1, Batch Index: 154- Eval test - q_loss = 0.02280 , q_risk_0.1 = 0.01012 , q_risk_0.5 = 0.02683 , q_risk_0.9 = 0.01368\n",
      "Epoch: 1, Batch Index: 200 - Train Loss = 0.02513308558613062\n",
      "Starting Epoch Index 2\n",
      "Evaluating train set\n",
      "Epoch: 2, Batch Index: 231- Eval train - q_loss = 0.01727 , q_risk_0.1 = 0.00795 , q_risk_0.5 = 0.01923 , q_risk_0.9 = 0.00988\n",
      "Evaluating validation set\n",
      "Epoch: 2, Batch Index: 231- Eval validation - q_loss = 0.01671 , q_risk_0.1 = 0.00867 , q_risk_0.5 = 0.01907 , q_risk_0.9 = 0.00949\n",
      "Evaluating test set\n",
      "Epoch: 2, Batch Index: 231- Eval test - q_loss = 0.01558 , q_risk_0.1 = 0.00762 , q_risk_0.5 = 0.01749 , q_risk_0.9 = 0.00952\n",
      "Epoch: 2, Batch Index: 250 - Train Loss = 0.021500964537262916\n",
      "Epoch: 2, Batch Index: 300 - Train Loss = 0.02037219386547804\n",
      "Starting Epoch Index 3\n",
      "Evaluating train set\n",
      "Epoch: 3, Batch Index: 308- Eval train - q_loss = 0.01612 , q_risk_0.1 = 0.00815 , q_risk_0.5 = 0.01674 , q_risk_0.9 = 0.01090\n",
      "Evaluating validation set\n",
      "Epoch: 3, Batch Index: 308- Eval validation - q_loss = 0.01553 , q_risk_0.1 = 0.00718 , q_risk_0.5 = 0.01714 , q_risk_0.9 = 0.01047\n",
      "Evaluating test set\n",
      "Epoch: 3, Batch Index: 308- Eval test - q_loss = 0.01628 , q_risk_0.1 = 0.00757 , q_risk_0.5 = 0.01835 , q_risk_0.9 = 0.01093\n",
      "Epoch: 3, Batch Index: 350 - Train Loss = 0.02040870601311326\n",
      "Starting Epoch Index 4\n",
      "Evaluating train set\n",
      "Epoch: 4, Batch Index: 385- Eval train - q_loss = 0.01563 , q_risk_0.1 = 0.00792 , q_risk_0.5 = 0.01706 , q_risk_0.9 = 0.00888\n",
      "Evaluating validation set\n",
      "Epoch: 4, Batch Index: 385- Eval validation - q_loss = 0.01903 , q_risk_0.1 = 0.00912 , q_risk_0.5 = 0.02053 , q_risk_0.9 = 0.01176\n",
      "Evaluating test set\n",
      "Epoch: 4, Batch Index: 385- Eval test - q_loss = 0.01943 , q_risk_0.1 = 0.00955 , q_risk_0.5 = 0.02184 , q_risk_0.9 = 0.01166\n",
      "Epoch: 4, Batch Index: 400 - Train Loss = 0.017509223129600286\n",
      "Epoch: 4, Batch Index: 450 - Train Loss = 0.018059907462447882\n",
      "Starting Epoch Index 5\n",
      "Evaluating train set\n",
      "Epoch: 5, Batch Index: 462- Eval train - q_loss = 0.01147 , q_risk_0.1 = 0.00566 , q_risk_0.5 = 0.01299 , q_risk_0.9 = 0.00777\n",
      "Evaluating validation set\n",
      "Epoch: 5, Batch Index: 462- Eval validation - q_loss = 0.01237 , q_risk_0.1 = 0.00660 , q_risk_0.5 = 0.01385 , q_risk_0.9 = 0.00682\n",
      "Evaluating test set\n",
      "Epoch: 5, Batch Index: 462- Eval test - q_loss = 0.01159 , q_risk_0.1 = 0.00633 , q_risk_0.5 = 0.01324 , q_risk_0.9 = 0.00622\n",
      "Epoch: 5, Batch Index: 500 - Train Loss = 0.015877957995980976\n",
      "Starting Epoch Index 6\n",
      "Evaluating train set\n",
      "Epoch: 6, Batch Index: 539- Eval train - q_loss = 0.01425 , q_risk_0.1 = 0.00642 , q_risk_0.5 = 0.01711 , q_risk_0.9 = 0.00765\n",
      "Evaluating validation set\n",
      "Epoch: 6, Batch Index: 539- Eval validation - q_loss = 0.01490 , q_risk_0.1 = 0.00674 , q_risk_0.5 = 0.01768 , q_risk_0.9 = 0.00845\n",
      "Evaluating test set\n",
      "Epoch: 6, Batch Index: 539- Eval test - q_loss = 0.01427 , q_risk_0.1 = 0.00670 , q_risk_0.5 = 0.01752 , q_risk_0.9 = 0.00767\n",
      "Epoch: 6, Batch Index: 550 - Train Loss = 0.015951833073049784\n",
      "Epoch: 6, Batch Index: 600 - Train Loss = 0.015510000176727773\n",
      "Starting Epoch Index 7\n",
      "Evaluating train set\n",
      "Epoch: 7, Batch Index: 616- Eval train - q_loss = 0.01871 , q_risk_0.1 = 0.00822 , q_risk_0.5 = 0.01989 , q_risk_0.9 = 0.01197\n",
      "Evaluating validation set\n",
      "Epoch: 7, Batch Index: 616- Eval validation - q_loss = 0.01824 , q_risk_0.1 = 0.00935 , q_risk_0.5 = 0.02047 , q_risk_0.9 = 0.01083\n",
      "Evaluating test set\n",
      "Epoch: 7, Batch Index: 616- Eval test - q_loss = 0.01707 , q_risk_0.1 = 0.00831 , q_risk_0.5 = 0.01883 , q_risk_0.9 = 0.01044\n",
      "Epoch: 7, Batch Index: 650 - Train Loss = 0.015935685634613037\n",
      "Starting Epoch Index 8\n",
      "Evaluating train set\n",
      "Epoch: 8, Batch Index: 693- Eval train - q_loss = 0.01170 , q_risk_0.1 = 0.00527 , q_risk_0.5 = 0.01292 , q_risk_0.9 = 0.00755\n",
      "Evaluating validation set\n",
      "Epoch: 8, Batch Index: 693- Eval validation - q_loss = 0.01384 , q_risk_0.1 = 0.00698 , q_risk_0.5 = 0.01569 , q_risk_0.9 = 0.00801\n",
      "Evaluating test set\n",
      "Epoch: 8, Batch Index: 693- Eval test - q_loss = 0.01331 , q_risk_0.1 = 0.00649 , q_risk_0.5 = 0.01582 , q_risk_0.9 = 0.00760\n",
      "Epoch: 8, Batch Index: 700 - Train Loss = 0.01543965982273221\n",
      "Epoch: 8, Batch Index: 750 - Train Loss = 0.014816926140338183\n",
      "Starting Epoch Index 9\n",
      "Evaluating train set\n",
      "Epoch: 9, Batch Index: 770- Eval train - q_loss = 0.01131 , q_risk_0.1 = 0.00610 , q_risk_0.5 = 0.01347 , q_risk_0.9 = 0.00528\n",
      "Evaluating validation set\n",
      "Epoch: 9, Batch Index: 770- Eval validation - q_loss = 0.01279 , q_risk_0.1 = 0.00739 , q_risk_0.5 = 0.01438 , q_risk_0.9 = 0.00682\n",
      "Evaluating test set\n",
      "Epoch: 9, Batch Index: 770- Eval test - q_loss = 0.01193 , q_risk_0.1 = 0.00672 , q_risk_0.5 = 0.01443 , q_risk_0.9 = 0.00557\n",
      "Epoch: 9, Batch Index: 800 - Train Loss = 0.013974089697003365\n",
      "Starting Epoch Index 10\n",
      "Evaluating train set\n",
      "Epoch: 10, Batch Index: 847- Eval train - q_loss = 0.00979 , q_risk_0.1 = 0.00467 , q_risk_0.5 = 0.01025 , q_risk_0.9 = 0.00674\n",
      "Evaluating validation set\n",
      "Epoch: 10, Batch Index: 847- Eval validation - q_loss = 0.01218 , q_risk_0.1 = 0.00664 , q_risk_0.5 = 0.01324 , q_risk_0.9 = 0.00728\n",
      "Evaluating test set\n",
      "Epoch: 10, Batch Index: 847- Eval test - q_loss = 0.01092 , q_risk_0.1 = 0.00563 , q_risk_0.5 = 0.01207 , q_risk_0.9 = 0.00685\n",
      "Epoch: 10, Batch Index: 850 - Train Loss = 0.014045241624116897\n",
      "Epoch: 10, Batch Index: 900 - Train Loss = 0.013917447784915566\n",
      "Starting Epoch Index 11\n",
      "Evaluating train set\n",
      "Epoch: 11, Batch Index: 924- Eval train - q_loss = 0.01158 , q_risk_0.1 = 0.00592 , q_risk_0.5 = 0.01346 , q_risk_0.9 = 0.00703\n",
      "Evaluating validation set\n",
      "Epoch: 11, Batch Index: 924- Eval validation - q_loss = 0.01277 , q_risk_0.1 = 0.00601 , q_risk_0.5 = 0.01495 , q_risk_0.9 = 0.00720\n",
      "Evaluating test set\n",
      "Epoch: 11, Batch Index: 924- Eval test - q_loss = 0.01231 , q_risk_0.1 = 0.00607 , q_risk_0.5 = 0.01474 , q_risk_0.9 = 0.00653\n",
      "Epoch: 11, Batch Index: 950 - Train Loss = 0.014107088781893254\n",
      "Epoch: 11, Batch Index: 1000 - Train Loss = 0.013415486719459295\n",
      "Starting Epoch Index 12\n",
      "Evaluating train set\n",
      "Epoch: 12, Batch Index: 1001- Eval train - q_loss = 0.01106 , q_risk_0.1 = 0.00742 , q_risk_0.5 = 0.01302 , q_risk_0.9 = 0.00469\n",
      "Evaluating validation set\n",
      "Epoch: 12, Batch Index: 1001- Eval validation - q_loss = 0.01395 , q_risk_0.1 = 0.00824 , q_risk_0.5 = 0.01619 , q_risk_0.9 = 0.00620\n",
      "Evaluating test set\n",
      "Epoch: 12, Batch Index: 1001- Eval test - q_loss = 0.01481 , q_risk_0.1 = 0.00838 , q_risk_0.5 = 0.01669 , q_risk_0.9 = 0.00760\n",
      "Epoch: 12, Batch Index: 1050 - Train Loss = 0.013608397487550973\n",
      "Starting Epoch Index 13\n",
      "Evaluating train set\n",
      "Epoch: 13, Batch Index: 1078- Eval train - q_loss = 0.01478 , q_risk_0.1 = 0.00697 , q_risk_0.5 = 0.01596 , q_risk_0.9 = 0.00965\n",
      "Evaluating validation set\n",
      "Epoch: 13, Batch Index: 1078- Eval validation - q_loss = 0.01482 , q_risk_0.1 = 0.00746 , q_risk_0.5 = 0.01673 , q_risk_0.9 = 0.00909\n",
      "Evaluating test set\n",
      "Epoch: 13, Batch Index: 1078- Eval test - q_loss = 0.01475 , q_risk_0.1 = 0.00753 , q_risk_0.5 = 0.01638 , q_risk_0.9 = 0.00950\n",
      "Epoch: 13, Batch Index: 1100 - Train Loss = 0.014693632237613202\n",
      "Epoch: 13, Batch Index: 1150 - Train Loss = 0.01427158024162054\n",
      "Starting Epoch Index 14\n",
      "Evaluating train set\n",
      "Epoch: 14, Batch Index: 1155- Eval train - q_loss = 0.01466 , q_risk_0.1 = 0.00680 , q_risk_0.5 = 0.01807 , q_risk_0.9 = 0.00719\n",
      "Evaluating validation set\n",
      "Epoch: 14, Batch Index: 1155- Eval validation - q_loss = 0.01565 , q_risk_0.1 = 0.00701 , q_risk_0.5 = 0.01851 , q_risk_0.9 = 0.00847\n",
      "Evaluating test set\n",
      "Epoch: 14, Batch Index: 1155- Eval test - q_loss = 0.01539 , q_risk_0.1 = 0.00686 , q_risk_0.5 = 0.01832 , q_risk_0.9 = 0.00853\n",
      "Epoch: 14, Batch Index: 1200 - Train Loss = 0.013743321001529694\n",
      "Starting Epoch Index 15\n",
      "Evaluating train set\n",
      "Epoch: 15, Batch Index: 1232- Eval train - q_loss = 0.01192 , q_risk_0.1 = 0.00594 , q_risk_0.5 = 0.01310 , q_risk_0.9 = 0.00632\n",
      "Evaluating validation set\n",
      "Epoch: 15, Batch Index: 1232- Eval validation - q_loss = 0.01430 , q_risk_0.1 = 0.00765 , q_risk_0.5 = 0.01493 , q_risk_0.9 = 0.00863\n",
      "Evaluating test set\n",
      "Epoch: 15, Batch Index: 1232- Eval test - q_loss = 0.01239 , q_risk_0.1 = 0.00659 , q_risk_0.5 = 0.01373 , q_risk_0.9 = 0.00698\n",
      "Epoch: 15, Batch Index: 1250 - Train Loss = 0.013776526115834712\n",
      "Epoch: 15, Batch Index: 1300 - Train Loss = 0.01331555025652051\n",
      "Starting Epoch Index 16\n",
      "Evaluating train set\n",
      "Epoch: 16, Batch Index: 1309- Eval train - q_loss = 0.01289 , q_risk_0.1 = 0.00650 , q_risk_0.5 = 0.01521 , q_risk_0.9 = 0.00701\n",
      "Evaluating validation set\n",
      "Epoch: 16, Batch Index: 1309- Eval validation - q_loss = 0.01416 , q_risk_0.1 = 0.00729 , q_risk_0.5 = 0.01570 , q_risk_0.9 = 0.00794\n",
      "Evaluating test set\n",
      "Epoch: 16, Batch Index: 1309- Eval test - q_loss = 0.01281 , q_risk_0.1 = 0.00673 , q_risk_0.5 = 0.01481 , q_risk_0.9 = 0.00735\n",
      "Epoch: 16, Batch Index: 1350 - Train Loss = 0.014022592790424823\n",
      "Starting Epoch Index 17\n",
      "Evaluating train set\n",
      "Epoch: 17, Batch Index: 1386- Eval train - q_loss = 0.00960 , q_risk_0.1 = 0.00593 , q_risk_0.5 = 0.01070 , q_risk_0.9 = 0.00434\n",
      "Evaluating validation set\n",
      "Epoch: 17, Batch Index: 1386- Eval validation - q_loss = 0.01296 , q_risk_0.1 = 0.00770 , q_risk_0.5 = 0.01396 , q_risk_0.9 = 0.00653\n",
      "Evaluating test set\n",
      "Epoch: 17, Batch Index: 1386- Eval test - q_loss = 0.01213 , q_risk_0.1 = 0.00683 , q_risk_0.5 = 0.01372 , q_risk_0.9 = 0.00651\n",
      "Epoch: 17, Batch Index: 1400 - Train Loss = 0.012921658065170049\n",
      "Epoch: 17, Batch Index: 1450 - Train Loss = 0.013375636981800198\n",
      "Starting Epoch Index 18\n",
      "Evaluating train set\n",
      "Epoch: 18, Batch Index: 1463- Eval train - q_loss = 0.00886 , q_risk_0.1 = 0.00422 , q_risk_0.5 = 0.01029 , q_risk_0.9 = 0.00593\n",
      "Evaluating validation set\n",
      "Epoch: 18, Batch Index: 1463- Eval validation - q_loss = 0.01093 , q_risk_0.1 = 0.00579 , q_risk_0.5 = 0.01231 , q_risk_0.9 = 0.00580\n",
      "Evaluating test set\n",
      "Epoch: 18, Batch Index: 1463- Eval test - q_loss = 0.00975 , q_risk_0.1 = 0.00455 , q_risk_0.5 = 0.01130 , q_risk_0.9 = 0.00571\n",
      "Epoch: 18, Batch Index: 1500 - Train Loss = 0.011471993755549192\n",
      "Starting Epoch Index 19\n",
      "Evaluating train set\n",
      "Epoch: 19, Batch Index: 1540- Eval train - q_loss = 0.01154 , q_risk_0.1 = 0.00560 , q_risk_0.5 = 0.01227 , q_risk_0.9 = 0.00734\n",
      "Evaluating validation set\n",
      "Epoch: 19, Batch Index: 1540- Eval validation - q_loss = 0.01162 , q_risk_0.1 = 0.00590 , q_risk_0.5 = 0.01276 , q_risk_0.9 = 0.00668\n",
      "Evaluating test set\n",
      "Epoch: 19, Batch Index: 1540- Eval test - q_loss = 0.01083 , q_risk_0.1 = 0.00528 , q_risk_0.5 = 0.01238 , q_risk_0.9 = 0.00655\n",
      "Epoch: 19, Batch Index: 1550 - Train Loss = 0.011214902102947235\n",
      "Epoch: 19, Batch Index: 1600 - Train Loss = 0.011540149115025997\n",
      "Starting Epoch Index 20\n",
      "Evaluating train set\n",
      "Epoch: 20, Batch Index: 1617- Eval train - q_loss = 0.00909 , q_risk_0.1 = 0.00515 , q_risk_0.5 = 0.01006 , q_risk_0.9 = 0.00411\n",
      "Evaluating validation set\n",
      "Epoch: 20, Batch Index: 1617- Eval validation - q_loss = 0.01212 , q_risk_0.1 = 0.00677 , q_risk_0.5 = 0.01323 , q_risk_0.9 = 0.00648\n",
      "Evaluating test set\n",
      "Epoch: 20, Batch Index: 1617- Eval test - q_loss = 0.01149 , q_risk_0.1 = 0.00647 , q_risk_0.5 = 0.01301 , q_risk_0.9 = 0.00567\n",
      "Epoch: 20, Batch Index: 1650 - Train Loss = 0.012517958283424377\n",
      "Starting Epoch Index 21\n",
      "Evaluating train set\n",
      "Epoch: 21, Batch Index: 1694- Eval train - q_loss = 0.01049 , q_risk_0.1 = 0.00515 , q_risk_0.5 = 0.01205 , q_risk_0.9 = 0.00638\n",
      "Evaluating validation set\n",
      "Epoch: 21, Batch Index: 1694- Eval validation - q_loss = 0.01313 , q_risk_0.1 = 0.00685 , q_risk_0.5 = 0.01522 , q_risk_0.9 = 0.00731\n",
      "Evaluating test set\n",
      "Epoch: 21, Batch Index: 1694- Eval test - q_loss = 0.01184 , q_risk_0.1 = 0.00625 , q_risk_0.5 = 0.01423 , q_risk_0.9 = 0.00631\n",
      "Epoch: 21, Batch Index: 1700 - Train Loss = 0.012103828601539135\n",
      "Epoch: 21, Batch Index: 1750 - Train Loss = 0.012379845716059207\n",
      "Starting Epoch Index 22\n",
      "Evaluating train set\n",
      "Epoch: 22, Batch Index: 1771- Eval train - q_loss = 0.00991 , q_risk_0.1 = 0.00509 , q_risk_0.5 = 0.01133 , q_risk_0.9 = 0.00512\n",
      "Evaluating validation set\n",
      "Epoch: 22, Batch Index: 1771- Eval validation - q_loss = 0.01271 , q_risk_0.1 = 0.00623 , q_risk_0.5 = 0.01439 , q_risk_0.9 = 0.00694\n",
      "Evaluating test set\n",
      "Epoch: 22, Batch Index: 1771- Eval test - q_loss = 0.01108 , q_risk_0.1 = 0.00557 , q_risk_0.5 = 0.01319 , q_risk_0.9 = 0.00593\n",
      "Epoch: 22, Batch Index: 1800 - Train Loss = 0.01143646014854312\n",
      "Starting Epoch Index 23\n",
      "Evaluating train set\n",
      "Epoch: 23, Batch Index: 1848- Eval train - q_loss = 0.00742 , q_risk_0.1 = 0.00388 , q_risk_0.5 = 0.00752 , q_risk_0.9 = 0.00482\n",
      "Evaluating validation set\n",
      "Epoch: 23, Batch Index: 1848- Eval validation - q_loss = 0.01032 , q_risk_0.1 = 0.00528 , q_risk_0.5 = 0.01127 , q_risk_0.9 = 0.00617\n",
      "Evaluating test set\n",
      "Epoch: 23, Batch Index: 1848- Eval test - q_loss = 0.00942 , q_risk_0.1 = 0.00499 , q_risk_0.5 = 0.01046 , q_risk_0.9 = 0.00576\n",
      "Epoch: 23, Batch Index: 1850 - Train Loss = 0.01188257109373808\n",
      "Epoch: 23, Batch Index: 1900 - Train Loss = 0.01225265596061945\n",
      "Starting Epoch Index 24\n",
      "Evaluating train set\n",
      "Epoch: 24, Batch Index: 1925- Eval train - q_loss = 0.00906 , q_risk_0.1 = 0.00430 , q_risk_0.5 = 0.01023 , q_risk_0.9 = 0.00638\n",
      "Evaluating validation set\n",
      "Epoch: 24, Batch Index: 1925- Eval validation - q_loss = 0.01037 , q_risk_0.1 = 0.00551 , q_risk_0.5 = 0.01102 , q_risk_0.9 = 0.00622\n",
      "Evaluating test set\n",
      "Epoch: 24, Batch Index: 1925- Eval test - q_loss = 0.00918 , q_risk_0.1 = 0.00447 , q_risk_0.5 = 0.01041 , q_risk_0.9 = 0.00544\n",
      "Epoch: 24, Batch Index: 1950 - Train Loss = 0.011494917031377554\n",
      "Epoch: 24, Batch Index: 2000 - Train Loss = 0.011461139731109142\n",
      "Starting Epoch Index 25\n",
      "Evaluating train set\n",
      "Epoch: 25, Batch Index: 2002- Eval train - q_loss = 0.00894 , q_risk_0.1 = 0.00556 , q_risk_0.5 = 0.00999 , q_risk_0.9 = 0.00436\n",
      "Evaluating validation set\n",
      "Epoch: 25, Batch Index: 2002- Eval validation - q_loss = 0.01186 , q_risk_0.1 = 0.00634 , q_risk_0.5 = 0.01349 , q_risk_0.9 = 0.00645\n",
      "Evaluating test set\n",
      "Epoch: 25, Batch Index: 2002- Eval test - q_loss = 0.01187 , q_risk_0.1 = 0.00613 , q_risk_0.5 = 0.01327 , q_risk_0.9 = 0.00713\n",
      "Epoch: 25, Batch Index: 2050 - Train Loss = 0.011404133327305317\n",
      "Starting Epoch Index 26\n",
      "Evaluating train set\n",
      "Epoch: 26, Batch Index: 2079- Eval train - q_loss = 0.01170 , q_risk_0.1 = 0.00562 , q_risk_0.5 = 0.01290 , q_risk_0.9 = 0.00718\n",
      "Evaluating validation set\n",
      "Epoch: 26, Batch Index: 2079- Eval validation - q_loss = 0.01386 , q_risk_0.1 = 0.00677 , q_risk_0.5 = 0.01611 , q_risk_0.9 = 0.00789\n",
      "Evaluating test set\n",
      "Epoch: 26, Batch Index: 2079- Eval test - q_loss = 0.01420 , q_risk_0.1 = 0.00686 , q_risk_0.5 = 0.01658 , q_risk_0.9 = 0.00877\n",
      "Epoch: 26, Batch Index: 2100 - Train Loss = 0.012431238386780024\n",
      "Epoch: 26, Batch Index: 2150 - Train Loss = 0.012394507471472025\n",
      "Starting Epoch Index 27\n",
      "Evaluating train set\n",
      "Epoch: 27, Batch Index: 2156- Eval train - q_loss = 0.00977 , q_risk_0.1 = 0.00499 , q_risk_0.5 = 0.01076 , q_risk_0.9 = 0.00576\n",
      "Evaluating validation set\n",
      "Epoch: 27, Batch Index: 2156- Eval validation - q_loss = 0.01230 , q_risk_0.1 = 0.00693 , q_risk_0.5 = 0.01327 , q_risk_0.9 = 0.00707\n",
      "Evaluating test set\n",
      "Epoch: 27, Batch Index: 2156- Eval test - q_loss = 0.01098 , q_risk_0.1 = 0.00609 , q_risk_0.5 = 0.01236 , q_risk_0.9 = 0.00586\n",
      "Epoch: 27, Batch Index: 2200 - Train Loss = 0.01199027018621564\n",
      "Starting Epoch Index 28\n",
      "Evaluating train set\n",
      "Epoch: 28, Batch Index: 2233- Eval train - q_loss = 0.00804 , q_risk_0.1 = 0.00421 , q_risk_0.5 = 0.00838 , q_risk_0.9 = 0.00456\n",
      "Evaluating validation set\n",
      "Epoch: 28, Batch Index: 2233- Eval validation - q_loss = 0.01165 , q_risk_0.1 = 0.00634 , q_risk_0.5 = 0.01308 , q_risk_0.9 = 0.00625\n",
      "Evaluating test set\n",
      "Epoch: 28, Batch Index: 2233- Eval test - q_loss = 0.01055 , q_risk_0.1 = 0.00537 , q_risk_0.5 = 0.01209 , q_risk_0.9 = 0.00594\n",
      "Epoch: 28, Batch Index: 2250 - Train Loss = 0.012070924863219262\n",
      "Epoch: 28, Batch Index: 2300 - Train Loss = 0.012146645858883857\n",
      "Starting Epoch Index 29\n",
      "Evaluating train set\n",
      "Epoch: 29, Batch Index: 2310- Eval train - q_loss = 0.00838 , q_risk_0.1 = 0.00393 , q_risk_0.5 = 0.00947 , q_risk_0.9 = 0.00524\n",
      "Evaluating validation set\n",
      "Epoch: 29, Batch Index: 2310- Eval validation - q_loss = 0.01197 , q_risk_0.1 = 0.00602 , q_risk_0.5 = 0.01375 , q_risk_0.9 = 0.00657\n",
      "Evaluating test set\n",
      "Epoch: 29, Batch Index: 2310- Eval test - q_loss = 0.01081 , q_risk_0.1 = 0.00479 , q_risk_0.5 = 0.01283 , q_risk_0.9 = 0.00684\n",
      "Epoch: 29, Batch Index: 2350 - Train Loss = 0.011583399856463075\n",
      "Starting Epoch Index 30\n",
      "Evaluating train set\n",
      "Epoch: 30, Batch Index: 2387- Eval train - q_loss = 0.00846 , q_risk_0.1 = 0.00476 , q_risk_0.5 = 0.00925 , q_risk_0.9 = 0.00515\n",
      "Evaluating validation set\n",
      "Epoch: 30, Batch Index: 2387- Eval validation - q_loss = 0.01079 , q_risk_0.1 = 0.00599 , q_risk_0.5 = 0.01165 , q_risk_0.9 = 0.00635\n",
      "Evaluating test set\n",
      "Epoch: 30, Batch Index: 2387- Eval test - q_loss = 0.01021 , q_risk_0.1 = 0.00545 , q_risk_0.5 = 0.01138 , q_risk_0.9 = 0.00602\n",
      "Epoch: 30, Batch Index: 2400 - Train Loss = 0.011681233197450638\n",
      "Epoch: 30, Batch Index: 2450 - Train Loss = 0.01205649385228753\n",
      "Starting Epoch Index 31\n",
      "Evaluating train set\n",
      "Epoch: 31, Batch Index: 2464- Eval train - q_loss = 0.00865 , q_risk_0.1 = 0.00409 , q_risk_0.5 = 0.00963 , q_risk_0.9 = 0.00611\n",
      "Evaluating validation set\n",
      "Epoch: 31, Batch Index: 2464- Eval validation - q_loss = 0.01045 , q_risk_0.1 = 0.00566 , q_risk_0.5 = 0.01133 , q_risk_0.9 = 0.00611\n",
      "Evaluating test set\n",
      "Epoch: 31, Batch Index: 2464- Eval test - q_loss = 0.00914 , q_risk_0.1 = 0.00446 , q_risk_0.5 = 0.01036 , q_risk_0.9 = 0.00552\n",
      "Epoch: 31, Batch Index: 2500 - Train Loss = 0.011217667423188686\n",
      "Starting Epoch Index 32\n",
      "Evaluating train set\n",
      "Epoch: 32, Batch Index: 2541- Eval train - q_loss = 0.01325 , q_risk_0.1 = 0.00479 , q_risk_0.5 = 0.01603 , q_risk_0.9 = 0.00809\n",
      "Evaluating validation set\n",
      "Epoch: 32, Batch Index: 2541- Eval validation - q_loss = 0.01234 , q_risk_0.1 = 0.00533 , q_risk_0.5 = 0.01505 , q_risk_0.9 = 0.00679\n",
      "Evaluating test set\n",
      "Epoch: 32, Batch Index: 2541- Eval test - q_loss = 0.01153 , q_risk_0.1 = 0.00446 , q_risk_0.5 = 0.01431 , q_risk_0.9 = 0.00702\n",
      "Epoch: 32, Batch Index: 2550 - Train Loss = 0.011208692695945502\n",
      "Epoch: 32, Batch Index: 2600 - Train Loss = 0.010982841840013862\n",
      "Starting Epoch Index 33\n",
      "Evaluating train set\n",
      "Epoch: 33, Batch Index: 2618- Eval train - q_loss = 0.00964 , q_risk_0.1 = 0.00578 , q_risk_0.5 = 0.01095 , q_risk_0.9 = 0.00473\n",
      "Evaluating validation set\n",
      "Epoch: 33, Batch Index: 2618- Eval validation - q_loss = 0.01214 , q_risk_0.1 = 0.00677 , q_risk_0.5 = 0.01377 , q_risk_0.9 = 0.00625\n",
      "Evaluating test set\n",
      "Epoch: 33, Batch Index: 2618- Eval test - q_loss = 0.01159 , q_risk_0.1 = 0.00619 , q_risk_0.5 = 0.01313 , q_risk_0.9 = 0.00622\n",
      "Performing early stopping...!\n"
     ]
    }
   ],
   "source": [
    "while epoch_idx < max_epochs:\n",
    "        print(f\"Starting Epoch Index {epoch_idx}\")\n",
    "\n",
    "        # evaluation round\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            # for each subset\n",
    "            for subset_name, subset_loader in zip(['train','validation','test'],[train_loader,validation_loader,test_loader]):\n",
    "                print(f\"Evaluating {subset_name} set\")\n",
    "\n",
    "                q_loss_vals, q_risk_vals = [],[] # used for aggregating performance along the evaluation round\n",
    "                for v in range(eval_iters):\n",
    "                    #print(v)\n",
    "                    # get batch\n",
    "                    batch = next(subset_loader)\n",
    "                    #batch = [item.to(device) for item in batch]\n",
    "                    # process batch\n",
    "                    batch_loss,batch_q_risk = process_batch(batch=batch,model=model,quantiles_tensor=quantiles_tensor,device=device)\n",
    "                    # accumulate performance\n",
    "                    q_loss_vals.append(batch_loss)\n",
    "                    q_risk_vals.append(batch_q_risk)\n",
    "                #print('done')\n",
    "                # aggregate and average\n",
    "                eval_loss = torch.stack(q_loss_vals).mean(axis=0)\n",
    "                eval_q_risk = torch.stack(q_risk_vals,axis=0).mean(axis=0)\n",
    "\n",
    "                # keep for feeding the early stopping mechanism\n",
    "                if subset_name == 'validation':\n",
    "                    validation_loss = eval_loss\n",
    "\n",
    "                # log performance\n",
    "                print(f\"Epoch: {epoch_idx}, Batch Index: {batch_idx}\" + \\\n",
    "                    f\"- Eval {subset_name} - \" + \\\n",
    "                    f\"q_loss = {eval_loss:.5f} , \" + \\\n",
    "                    \" , \".join([f\"q_risk_{q:.1} = {risk:.5f}\" for q,risk in zip(quantiles_tensor,eval_q_risk)]))\n",
    "\n",
    "                # log metrics to wandb\n",
    "                wandb.log({\"eval_q_loss\": eval_loss})\n",
    "        # switch to training mode\n",
    "        model.train()\n",
    "\n",
    "        # update early stopping mechanism and stop if triggered\n",
    "        if es.step(validation_loss):\n",
    "            print('Performing early stopping...!')\n",
    "            break\n",
    "\n",
    "        # initiating a training round\n",
    "        for _ in range(epoch_iters):\n",
    "            # get training batch\n",
    "            batch = next(train_loader)\n",
    "\n",
    "            opt.zero_grad()\n",
    "            # process batch\n",
    "            loss,_ = process_batch(batch=batch,\n",
    "                                model=model,\n",
    "                                quantiles_tensor=quantiles_tensor,\n",
    "                                device=device)\n",
    "            \n",
    "            # Debug gradient norms\n",
    "            total_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            # compute gradients\n",
    "            loss.backward()\n",
    "            # update weights\n",
    "            opt.step()\n",
    "            # gradient clipping\n",
    "            \n",
    "            #torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "\n",
    "            # accumulate performance\n",
    "            loss_aggregator.append(loss.item())\n",
    "\n",
    "            # log performance\n",
    "            if batch_idx % log_interval == 0:\n",
    "                print(f\"Epoch: {epoch_idx}, Batch Index: {batch_idx} - Train Loss = {np.mean(loss_aggregator.get())}\")\n",
    "\n",
    "            # completed batch\n",
    "            batch_idx += 1\n",
    "            \n",
    "        scheduler.step(validation_loss)\n",
    "        \n",
    "        #Log the learning rate\n",
    "        current_lr = opt.param_groups[0]['lr']\n",
    "\n",
    "        wandb.log({\"learning_rate\": current_lr})\n",
    "            \n",
    "        # log metrics to wandb\n",
    "        wandb.log({\"train_loss\": np.mean(loss_aggregator.get())})\n",
    "\n",
    "        # completed epoch\n",
    "        epoch_idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "534703af-fe04-4cfb-9223-36c58c93e384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training over.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval_q_loss</td><td>██▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>learning_rate</td><td>████████████████▃▃▃▃▃▃▃▃▃▃▃▃▃▁▁▁▁</td></tr><tr><td>train_loss</td><td>█▅▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▂▁▁▁▁▂▁▁▂▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval_q_loss</td><td>0.01159</td></tr><tr><td>learning_rate</td><td>3e-05</td></tr><tr><td>train_loss</td><td>0.01164</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">curious-jazz-2</strong> at: <a href='https://wandb.ai/ayalahlou/TL_US_MMS/runs/2erifkce' target=\"_blank\">https://wandb.ai/ayalahlou/TL_US_MMS/runs/2erifkce</a><br> View project at: <a href='https://wandb.ai/ayalahlou/TL_US_MMS' target=\"_blank\">https://wandb.ai/ayalahlou/TL_US_MMS</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250608_031559-2erifkce/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Save model weights to a file\n",
    "print('Training over.')\n",
    "torch.save(model.state_dict(), output_path)\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db4a987-8713-4cd5-89b4-ea19bfa8a7a7",
   "metadata": {},
   "source": [
    "# Predict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "df21f06c-2b52-4c6c-92b9-85ea32c4672a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TemporalFusionTransformer(\n",
       "  (static_transform): InputChannelEmbedding(\n",
       "    (numeric_transform): NumericInputTransformation(\n",
       "      (numeric_projection_layers): ModuleList(\n",
       "        (0-1): 2 x Linear(in_features=1, out_features=160, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (categorical_transform): NullTransform()\n",
       "  )\n",
       "  (historical_ts_transform): InputChannelEmbedding(\n",
       "    (numeric_transform): TimeDistributed(\n",
       "      (module): NumericInputTransformation(\n",
       "        (numeric_projection_layers): ModuleList(\n",
       "          (0-6): 7 x Linear(in_features=1, out_features=160, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (categorical_transform): NullTransform()\n",
       "  )\n",
       "  (future_ts_transform): InputChannelEmbedding(\n",
       "    (numeric_transform): TimeDistributed(\n",
       "      (module): NumericInputTransformation(\n",
       "        (numeric_projection_layers): ModuleList(\n",
       "          (0-5): 6 x Linear(in_features=1, out_features=160, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (categorical_transform): NullTransform()\n",
       "  )\n",
       "  (static_selection): VariableSelectionNetwork(\n",
       "    (flattened_grn): GatedResidualNetwork(\n",
       "      (skip_layer): TimeDistributed(\n",
       "        (module): Linear(in_features=320, out_features=2, bias=True)\n",
       "      )\n",
       "      (fc1): TimeDistributed(\n",
       "        (module): Linear(in_features=320, out_features=160, bias=True)\n",
       "      )\n",
       "      (elu1): ELU(alpha=1.0)\n",
       "      (fc2): TimeDistributed(\n",
       "        (module): Linear(in_features=160, out_features=2, bias=True)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.2, inplace=False)\n",
       "      (gate): TimeDistributed(\n",
       "        (module): GatedLinearUnit(\n",
       "          (fc1): Linear(in_features=2, out_features=2, bias=True)\n",
       "          (fc2): Linear(in_features=2, out_features=2, bias=True)\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "      (layernorm): TimeDistributed(\n",
       "        (module): LayerNorm((2,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (softmax): Softmax(dim=1)\n",
       "    (single_variable_grns): ModuleList(\n",
       "      (0-1): 2 x GatedResidualNetwork(\n",
       "        (fc1): TimeDistributed(\n",
       "          (module): Linear(in_features=160, out_features=160, bias=True)\n",
       "        )\n",
       "        (elu1): ELU(alpha=1.0)\n",
       "        (fc2): TimeDistributed(\n",
       "          (module): Linear(in_features=160, out_features=160, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "        (gate): TimeDistributed(\n",
       "          (module): GatedLinearUnit(\n",
       "            (fc1): Linear(in_features=160, out_features=160, bias=True)\n",
       "            (fc2): Linear(in_features=160, out_features=160, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (layernorm): TimeDistributed(\n",
       "          (module): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (historical_ts_selection): VariableSelectionNetwork(\n",
       "    (flattened_grn): GatedResidualNetwork(\n",
       "      (skip_layer): TimeDistributed(\n",
       "        (module): Linear(in_features=1120, out_features=7, bias=True)\n",
       "      )\n",
       "      (fc1): TimeDistributed(\n",
       "        (module): Linear(in_features=1120, out_features=160, bias=True)\n",
       "      )\n",
       "      (context_projection): TimeDistributed(\n",
       "        (module): Linear(in_features=160, out_features=160, bias=False)\n",
       "      )\n",
       "      (elu1): ELU(alpha=1.0)\n",
       "      (fc2): TimeDistributed(\n",
       "        (module): Linear(in_features=160, out_features=7, bias=True)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.2, inplace=False)\n",
       "      (gate): TimeDistributed(\n",
       "        (module): GatedLinearUnit(\n",
       "          (fc1): Linear(in_features=7, out_features=7, bias=True)\n",
       "          (fc2): Linear(in_features=7, out_features=7, bias=True)\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "      (layernorm): TimeDistributed(\n",
       "        (module): LayerNorm((7,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (softmax): Softmax(dim=1)\n",
       "    (single_variable_grns): ModuleList(\n",
       "      (0-6): 7 x GatedResidualNetwork(\n",
       "        (fc1): TimeDistributed(\n",
       "          (module): Linear(in_features=160, out_features=160, bias=True)\n",
       "        )\n",
       "        (elu1): ELU(alpha=1.0)\n",
       "        (fc2): TimeDistributed(\n",
       "          (module): Linear(in_features=160, out_features=160, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "        (gate): TimeDistributed(\n",
       "          (module): GatedLinearUnit(\n",
       "            (fc1): Linear(in_features=160, out_features=160, bias=True)\n",
       "            (fc2): Linear(in_features=160, out_features=160, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (layernorm): TimeDistributed(\n",
       "          (module): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (future_ts_selection): VariableSelectionNetwork(\n",
       "    (flattened_grn): GatedResidualNetwork(\n",
       "      (skip_layer): TimeDistributed(\n",
       "        (module): Linear(in_features=960, out_features=6, bias=True)\n",
       "      )\n",
       "      (fc1): TimeDistributed(\n",
       "        (module): Linear(in_features=960, out_features=160, bias=True)\n",
       "      )\n",
       "      (context_projection): TimeDistributed(\n",
       "        (module): Linear(in_features=160, out_features=160, bias=False)\n",
       "      )\n",
       "      (elu1): ELU(alpha=1.0)\n",
       "      (fc2): TimeDistributed(\n",
       "        (module): Linear(in_features=160, out_features=6, bias=True)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.2, inplace=False)\n",
       "      (gate): TimeDistributed(\n",
       "        (module): GatedLinearUnit(\n",
       "          (fc1): Linear(in_features=6, out_features=6, bias=True)\n",
       "          (fc2): Linear(in_features=6, out_features=6, bias=True)\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "      (layernorm): TimeDistributed(\n",
       "        (module): LayerNorm((6,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (softmax): Softmax(dim=1)\n",
       "    (single_variable_grns): ModuleList(\n",
       "      (0-5): 6 x GatedResidualNetwork(\n",
       "        (fc1): TimeDistributed(\n",
       "          (module): Linear(in_features=160, out_features=160, bias=True)\n",
       "        )\n",
       "        (elu1): ELU(alpha=1.0)\n",
       "        (fc2): TimeDistributed(\n",
       "          (module): Linear(in_features=160, out_features=160, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "        (gate): TimeDistributed(\n",
       "          (module): GatedLinearUnit(\n",
       "            (fc1): Linear(in_features=160, out_features=160, bias=True)\n",
       "            (fc2): Linear(in_features=160, out_features=160, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (layernorm): TimeDistributed(\n",
       "          (module): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (static_encoder_selection): GatedResidualNetwork(\n",
       "    (fc1): TimeDistributed(\n",
       "      (module): Linear(in_features=160, out_features=160, bias=True)\n",
       "    )\n",
       "    (elu1): ELU(alpha=1.0)\n",
       "    (fc2): TimeDistributed(\n",
       "      (module): Linear(in_features=160, out_features=160, bias=True)\n",
       "    )\n",
       "    (dropout): Dropout(p=0.2, inplace=False)\n",
       "    (gate): TimeDistributed(\n",
       "      (module): GatedLinearUnit(\n",
       "        (fc1): Linear(in_features=160, out_features=160, bias=True)\n",
       "        (fc2): Linear(in_features=160, out_features=160, bias=True)\n",
       "        (sigmoid): Sigmoid()\n",
       "      )\n",
       "    )\n",
       "    (layernorm): TimeDistributed(\n",
       "      (module): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (static_encoder_enrichment): GatedResidualNetwork(\n",
       "    (fc1): TimeDistributed(\n",
       "      (module): Linear(in_features=160, out_features=160, bias=True)\n",
       "    )\n",
       "    (elu1): ELU(alpha=1.0)\n",
       "    (fc2): TimeDistributed(\n",
       "      (module): Linear(in_features=160, out_features=160, bias=True)\n",
       "    )\n",
       "    (dropout): Dropout(p=0.2, inplace=False)\n",
       "    (gate): TimeDistributed(\n",
       "      (module): GatedLinearUnit(\n",
       "        (fc1): Linear(in_features=160, out_features=160, bias=True)\n",
       "        (fc2): Linear(in_features=160, out_features=160, bias=True)\n",
       "        (sigmoid): Sigmoid()\n",
       "      )\n",
       "    )\n",
       "    (layernorm): TimeDistributed(\n",
       "      (module): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (static_encoder_sequential_cell_init): GatedResidualNetwork(\n",
       "    (fc1): TimeDistributed(\n",
       "      (module): Linear(in_features=160, out_features=160, bias=True)\n",
       "    )\n",
       "    (elu1): ELU(alpha=1.0)\n",
       "    (fc2): TimeDistributed(\n",
       "      (module): Linear(in_features=160, out_features=160, bias=True)\n",
       "    )\n",
       "    (dropout): Dropout(p=0.2, inplace=False)\n",
       "    (gate): TimeDistributed(\n",
       "      (module): GatedLinearUnit(\n",
       "        (fc1): Linear(in_features=160, out_features=160, bias=True)\n",
       "        (fc2): Linear(in_features=160, out_features=160, bias=True)\n",
       "        (sigmoid): Sigmoid()\n",
       "      )\n",
       "    )\n",
       "    (layernorm): TimeDistributed(\n",
       "      (module): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (static_encoder_sequential_state_init): GatedResidualNetwork(\n",
       "    (fc1): TimeDistributed(\n",
       "      (module): Linear(in_features=160, out_features=160, bias=True)\n",
       "    )\n",
       "    (elu1): ELU(alpha=1.0)\n",
       "    (fc2): TimeDistributed(\n",
       "      (module): Linear(in_features=160, out_features=160, bias=True)\n",
       "    )\n",
       "    (dropout): Dropout(p=0.2, inplace=False)\n",
       "    (gate): TimeDistributed(\n",
       "      (module): GatedLinearUnit(\n",
       "        (fc1): Linear(in_features=160, out_features=160, bias=True)\n",
       "        (fc2): Linear(in_features=160, out_features=160, bias=True)\n",
       "        (sigmoid): Sigmoid()\n",
       "      )\n",
       "    )\n",
       "    (layernorm): TimeDistributed(\n",
       "      (module): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (past_lstm): LSTM(160, 160, num_layers=4, batch_first=True, dropout=0.2)\n",
       "  (future_lstm): LSTM(160, 160, num_layers=4, batch_first=True, dropout=0.2)\n",
       "  (post_lstm_gating): GateAddNorm(\n",
       "    (dropout_layer): Dropout(p=0.2, inplace=False)\n",
       "    (gate): TimeDistributed(\n",
       "      (module): GatedLinearUnit(\n",
       "        (fc1): Linear(in_features=160, out_features=160, bias=True)\n",
       "        (fc2): Linear(in_features=160, out_features=160, bias=True)\n",
       "        (sigmoid): Sigmoid()\n",
       "      )\n",
       "    )\n",
       "    (layernorm): TimeDistributed(\n",
       "      (module): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (static_enrichment_grn): GatedResidualNetwork(\n",
       "    (fc1): TimeDistributed(\n",
       "      (module): Linear(in_features=160, out_features=160, bias=True)\n",
       "    )\n",
       "    (context_projection): TimeDistributed(\n",
       "      (module): Linear(in_features=160, out_features=160, bias=False)\n",
       "    )\n",
       "    (elu1): ELU(alpha=1.0)\n",
       "    (fc2): TimeDistributed(\n",
       "      (module): Linear(in_features=160, out_features=160, bias=True)\n",
       "    )\n",
       "    (dropout): Dropout(p=0.2, inplace=False)\n",
       "    (gate): TimeDistributed(\n",
       "      (module): GatedLinearUnit(\n",
       "        (fc1): Linear(in_features=160, out_features=160, bias=True)\n",
       "        (fc2): Linear(in_features=160, out_features=160, bias=True)\n",
       "        (sigmoid): Sigmoid()\n",
       "      )\n",
       "    )\n",
       "    (layernorm): TimeDistributed(\n",
       "      (module): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (multihead_attn): InterpretableMultiHeadAttention(\n",
       "    (w_q): Linear(in_features=160, out_features=640, bias=True)\n",
       "    (w_k): Linear(in_features=160, out_features=640, bias=True)\n",
       "    (w_v): Linear(in_features=160, out_features=160, bias=True)\n",
       "    (out): Linear(in_features=160, out_features=160, bias=True)\n",
       "  )\n",
       "  (post_attention_gating): GateAddNorm(\n",
       "    (dropout_layer): Dropout(p=0.2, inplace=False)\n",
       "    (gate): TimeDistributed(\n",
       "      (module): GatedLinearUnit(\n",
       "        (fc1): Linear(in_features=160, out_features=160, bias=True)\n",
       "        (fc2): Linear(in_features=160, out_features=160, bias=True)\n",
       "        (sigmoid): Sigmoid()\n",
       "      )\n",
       "    )\n",
       "    (layernorm): TimeDistributed(\n",
       "      (module): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (pos_wise_ff_grn): GatedResidualNetwork(\n",
       "    (fc1): TimeDistributed(\n",
       "      (module): Linear(in_features=160, out_features=160, bias=True)\n",
       "    )\n",
       "    (elu1): ELU(alpha=1.0)\n",
       "    (fc2): TimeDistributed(\n",
       "      (module): Linear(in_features=160, out_features=160, bias=True)\n",
       "    )\n",
       "    (dropout): Dropout(p=0.2, inplace=False)\n",
       "    (gate): TimeDistributed(\n",
       "      (module): GatedLinearUnit(\n",
       "        (fc1): Linear(in_features=160, out_features=160, bias=True)\n",
       "        (fc2): Linear(in_features=160, out_features=160, bias=True)\n",
       "        (sigmoid): Sigmoid()\n",
       "      )\n",
       "    )\n",
       "    (layernorm): TimeDistributed(\n",
       "      (module): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (pos_wise_ff_gating): GateAddNorm(\n",
       "    (gate): TimeDistributed(\n",
       "      (module): GatedLinearUnit(\n",
       "        (fc1): Linear(in_features=160, out_features=160, bias=True)\n",
       "        (fc2): Linear(in_features=160, out_features=160, bias=True)\n",
       "        (sigmoid): Sigmoid()\n",
       "      )\n",
       "    )\n",
       "    (layernorm): TimeDistributed(\n",
       "      (module): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (output_layer): Linear(in_features=160, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_path=\"/home/jovyan/phenology-ml-clm/data/predictions/USMMS_06082025.pkl\"\n",
    "model.eval() # switch to evaluation mode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "64395636-e813-4745-b444-9c69610dea8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:00<00:00, 59.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving in /home/jovyan/phenology-ml-clm/data/predictions/USMMS_06082025.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "output_aggregator = dict() # will be used for aggregating the outputs across batches\n",
    "with torch.no_grad():\n",
    "    # go over the batches of the serial data loader\n",
    "    for batch in tqdm(test_serial_loader):# change this from validation serial loader\n",
    "        # process each batch\n",
    "        if is_cuda:\n",
    "            for k in list(batch.keys()):\n",
    "                batch[k] = batch[k].to(device)\n",
    "        batch_outputs = model(batch)\n",
    "\n",
    "        # accumulate outputs, as well as labels\n",
    "        for output_key,output_tensor in batch_outputs.items():\n",
    "            output_aggregator.setdefault(output_key,[]).append(output_tensor.cpu().numpy())\n",
    "        \n",
    "validation_outputs = dict()\n",
    "for k in list(output_aggregator.keys()):\n",
    "    validation_outputs[k] = np.concatenate(output_aggregator[k],axis=0)\n",
    "\n",
    "# Save the dictionary using Pickle\n",
    "with open(prediction_path, \"wb\") as pickle_file:\n",
    "    print(\"saving in\", prediction_path)\n",
    "    pickle.dump(validation_outputs, pickle_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875eb554-cae5-456e-ba19-edb10db6974d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
