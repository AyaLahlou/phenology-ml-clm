{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da0d474f",
   "metadata": {},
   "source": [
    "# Fine-tune TFT on US MMS Data\n",
    "This notebook fine-tunes a pretrained Temporal Fusion Transformer using the US MMS dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181cb9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "# Import the TemporalFusionTransformer from the local codebase\n",
    "# This assumes a module defining the model exists in src.tft_model\n",
    "# Replace the path below with the correct one if different\n",
    "try:\n",
    "    from src.tft_model import TemporalFusionTransformer\n",
    "except ImportError:\n",
    "    from src.model_training import TemporalFusionTransformer  # fallback if model defined here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd048c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MMSDataset(Dataset):\n",
    "    \"\"\"Dataset for US MMS data prepared for TFT fine-tuning.\n",
    "\n",
    "    We assume the CSV already contains windows of length `history_len` for each\n",
    "    known attribute. Each attribute has columns formatted as\n",
    "    `<attr>_0`, `<attr>_1`, ..., `<attr>_<history_len-1>`. Static attributes are\n",
    "    provided once per row and the target column is `onset_doy`.\n",
    "    \"\"\"\n",
    "\n",
    "    known_attrs = ['tmin', 'tmax', 'radiation', 'precipitation', 'sm', 'photoperiod']\n",
    "    static_attrs = ['latitude', 'longitude']\n",
    "\n",
    "    def __init__(self, csv_path: str, history_len: int = 90):\n",
    "        super().__init__()\n",
    "        self.history_len = history_len\n",
    "        self.df = pd.read_csv(csv_path)\n",
    "        # If there is a date column, ensure the dataframe is sorted\n",
    "        if 'date' in self.df.columns:\n",
    "            self.df = self.df.sort_values('date')\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        row = self.df.iloc[idx]\n",
    "        # collect historical known attributes\n",
    "        ts_values = []\n",
    "        for attr in self.known_attrs:\n",
    "            for t in range(self.history_len):\n",
    "                col = f\"{attr}_{t}\"\n",
    "                ts_values.append(row[col])\n",
    "        # reshape to (history_len, n_features)\n",
    "        ts_array = np.array(ts_values, dtype=np.float32).reshape(self.history_len, -1)\n",
    "        static_array = row[self.static_attrs].values.astype(np.float32)\n",
    "        target = np.float32(row['onset_doy'])\n",
    "        return (\n",
    "            torch.from_numpy(ts_array),\n",
    "            torch.from_numpy(static_array),\n",
    "            torch.tensor(target)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edadeb8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, optimizer, criterion, device, epoch):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for batch_idx, (ts, static, target) in enumerate(dataloader):\n",
    "        ts = ts.to(device)\n",
    "        static = static.to(device)\n",
    "        target = target.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(ts, static)\n",
    "        loss = criterion(output.squeeze(), target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if (batch_idx + 1) % 50 == 0:\n",
    "            avg_loss = running_loss / 50\n",
    "            print(f\"Epoch {epoch}, Batch {batch_idx+1}, Loss = {avg_loss:.4f}\")\n",
    "            running_loss = 0.0\n",
    "\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    count = 0\n",
    "    with torch.no_grad():\n",
    "        for ts, static, target in dataloader:\n",
    "            ts = ts.to(device)\n",
    "            static = static.to(device)\n",
    "            target = target.to(device)\n",
    "            output = model(ts, static)\n",
    "            loss = criterion(output.squeeze(), target)\n",
    "            total_loss += loss.item()\n",
    "            count += 1\n",
    "    return total_loss / max(count, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3021c2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    print(\"Loading dataset from\", args.data_csv)\n",
    "    dataset = MMSDataset(args.data_csv, history_len=90)\n",
    "\n",
    "    # simple train/val split (80/20)\n",
    "    val_size = int(0.2 * len(dataset))\n",
    "    train_size = len(dataset) - val_size\n",
    "    train_ds, val_ds = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=8, shuffle=True, num_workers=2)\n",
    "    val_loader = DataLoader(val_ds, batch_size=8, shuffle=False, num_workers=2)\n",
    "\n",
    "    # Instantiate model\n",
    "    model = TemporalFusionTransformer(\n",
    "        seq_len=90,\n",
    "        n_time_features=6,\n",
    "        n_static_features=2,\n",
    "        n_categorical_features=0,\n",
    "        future_len=10,\n",
    "    )\n",
    "    checkpoint_path = args.checkpoint\n",
    "    print(f\"Loaded pretrained TFT from {checkpoint_path}\")\n",
    "    state_dict = torch.load(checkpoint_path, map_location=device)\n",
    "    model.load_state_dict(state_dict)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # Freeze all parameters except the output layer\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    for param in model.output_layer.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "    optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-4)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    print(\"Starting fine-tuning on US MMS dataset\")\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        train(model, train_loader, optimizer, criterion, device, epoch)\n",
    "        val_loss = evaluate(model, val_loader, criterion, device)\n",
    "        print(f\"Epoch {epoch} validation loss: {val_loss:.4f}\")\n",
    "\n",
    "    # Save checkpoint\n",
    "    model.eval()\n",
    "    os.makedirs(os.path.dirname(args.output_checkpoint), exist_ok=True)\n",
    "    torch.save(model.state_dict(), args.output_checkpoint)\n",
    "    print(f\"Saved fine-tuned model to {args.output_checkpoint}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description=\"Fine-tune pretrained TFT on US MMS data\")\n",
    "    parser.add_argument(\"--checkpoint\", type=str, required=True, help=\"Path to pretrained model checkpoint\")\n",
    "    parser.add_argument(\"--data_csv\", type=str, required=True, help=\"Path to preprocessed MMS CSV file\")\n",
    "    parser.add_argument(\"--output_checkpoint\", type=str, required=True, help=\"Output path for fine-tuned model\")\n",
    "    parser.add_argument(\"--epochs\", type=int, default=3, help=\"Number of fine-tuning epochs\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d08f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser(description='Fine-tune pretrained TFT on US MMS data')\n",
    "    parser.add_argument('--checkpoint', type=str, required=True, help='Path to pretrained model checkpoint')\n",
    "    parser.add_argument('--data_csv', type=str, required=True, help='Path to preprocessed MMS CSV file')\n",
    "    parser.add_argument('--output_checkpoint', type=str, required=True, help='Output path for fine-tuned model')\n",
    "    parser.add_argument('--epochs', type=int, default=3, help='Number of fine-tuning epochs')\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    main(args)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
